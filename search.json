[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metagenomics bioinformatics at MGnify (2024)",
    "section": "",
    "text": "This is the course material for the Metagenomics bioinformatics at MGnify course (2024).\n\nOverview\nGain knowledge of the tools, processes and analysis approaches used in the field of metagenomics.\nThis course will cover the metagenomics data analysis workflow from the point of newly generated sequence data. Participants will explore the use of publicly available resources and tools to manage, share, analyse and interpret metagenomics data. The content will include issues of data quality control and how to submit to public repositories. While sessions will detail marker-gene and whole-genome shotgun (WGS) approaches; the primary focus will be on assembly-based approaches. Discussions will also explore considerations when assembling genome data, the analysis that can be carried out by MGnify on such datasets, and what downstream analysis options and tools are available"
  },
  {
    "objectID": "sessions/virify.html",
    "href": "sessions/virify.html",
    "title": "Viral detection and classification",
    "section": "",
    "text": "These instructions are for the course VM. To run externally please see the section at the end.\n\n\n\n\n\n\nStep\n\n\n\nTo begin the practical we need to setup our docker container. Change into the virify_tutorial directory and setup the environment by running the following commands in your current terminal session:\n    cp -r /media/penelopeCloud/virify_tutorial /home/training\n    cp /media/penelopeCloud/virify_tutorial/2022-11-01_ete3_ncbi_tax.sqlite /home/training/virify_tutorial/databases\n    cd /home/training/virify_tutorial\n    chmod -R 777 /home/training/virify_tutorial\n    docker pull quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    docker run --rm -it -v $(pwd):/opt/data quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    mkdir obs_results\n\n\nAll commands detailed below will be run from within this current working directory. Note: if there are any issues in running this tutorial, there is a separate directory exp_results/ with pre-computed results.",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/virify.html#prerequisites",
    "href": "sessions/virify.html#prerequisites",
    "title": "Viral detection and classification",
    "section": "",
    "text": "These instructions are for the course VM. To run externally please see the section at the end.\n\n\n\n\n\n\nStep\n\n\n\nTo begin the practical we need to setup our docker container. Change into the virify_tutorial directory and setup the environment by running the following commands in your current terminal session:\n    cp -r /media/penelopeCloud/virify_tutorial /home/training\n    cp /media/penelopeCloud/virify_tutorial/2022-11-01_ete3_ncbi_tax.sqlite /home/training/virify_tutorial/databases\n    cd /home/training/virify_tutorial\n    chmod -R 777 /home/training/virify_tutorial\n    docker pull quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    docker run --rm -it -v $(pwd):/opt/data quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    mkdir obs_results\n\n\nAll commands detailed below will be run from within this current working directory. Note: if there are any issues in running this tutorial, there is a separate directory exp_results/ with pre-computed results.",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/virify.html#identification-of-putative-viral-sequences",
    "href": "sessions/virify.html#identification-of-putative-viral-sequences",
    "title": "Viral detection and classification",
    "section": "1. Identification of putative viral sequences",
    "text": "1. Identification of putative viral sequences\nIn order to retrieve putative viral sequences from a set of metagenomic contigs, we are going to use two different tools designed for this purpose, each of which employs a different strategy for viral sequence detection: VirFinder and VirSorter. VirFinder uses a prediction model based on kmer profiles trained using a reference database of viral and prokaryotic sequences. In contrast, VirSorter mainly relies on the comparison of predicted proteins with a comprehensive database of viral proteins and profile HMMs. The VIRify pipeline uses both tools as they provide complementary results:\nVirFinder performs better than VirSorter for short contigs (&lt;3kb) and includes a prediction model suitable for detecting both eukaryotic and prokaryotic viruses (phages). In addition to reporting the presence of phage contigs, VirSorter detects and reports the presence of prophage sequences (phages integrated in contigs containing their prokaryotic hosts).\n\n\n\n\n\n\nStep\n\n\n\nIn the current working directory, you will find the metagenomic assembly we will be working with ERR575691_host_filtered_filt500bp_renamed.fasta.\nThe typical contig input will have host sequences removed and be filtered to a minimum contig length of 500bp. The contigs were also renamed, as VirSorter has trouble with longer contig names. A mapping file with the original contig names is provided ERR575691_host_filtered_filt500bp_map.tsv.\nWe will now filter the contigs further to keep only those that are ≥1.5 kb, by using the custom python script filter_contigs_len.py as follows:\n    filter_contigs_len.py -f ERR575691_host_filtered_filt500bp_renamed.fasta -l 1.5 -o obs_results\nThe output from this command is a file named ERR575691_host_filtered_filt500bp_renamed_filt1500bp.fasta which is located in the obs_results directory. Our dataset is now ready to be processed for the detection of putative viral sequences. We will first analyse it with VirFinder using a custom R script and select a subset of columns from the output:\n    run_virfinder.Rscript databases/VF.modEPV_k8.rda obs_results/ERR575691_host_filtered_filt500bp_renamed_filt1500bp.fasta obs_results\n    awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4}' obs_results/ERR575691_host_filtered_filt500bp_renamed_filt1500bp_virfinder_all.tsv  &gt; obs_results/ERR575691_virfinder.txt\n\n\n\n\n\n\n\n\nTip\n\n\n\nLets look at the outputs of VirFinder. Look at the plot in the image below.\nAs you can see there is a relationship between the p-value and the score. A higher score or lower p-value indicates a higher likelihood of the sequence being a viral sequence. You will also notice that the results correlate with the contig length. The curves are slightly different depending on whether the contigs are &gt; or &lt; than 3kb. This is because VirFinder uses different machine learning models at these different levels of length.\n\n\n\n\n\nVirFinder scores\n\n\nYou will see a tabular file obs_results/ERR575691_virfinder.txt that collates the results obtained for each contig from the processed FASTA file.\n\n\n\n\n\n\nStep\n\n\n\nThe next step will be to analyse the metagenomic assembly using VirSorter. This can take a while to run so it has been done for you. We will copy the results to our output directory.\n    cp -r exp_results/virsorter_output obs_results/\n\n\nIf you wish to run this anytime after the practical, you will need to download the VirSorter database into the data/databases folder and then the following command can be used:\n#DON'T RUN NOW\nwrapper_phage_contigs_sorter_iPlant.pl -f obs_results/ERR575691_host_filtered_filt500bp_renamed_filt1500bp.fasta --db 2 --wdir obs_results/virsorter_output --virome --data-dir /opt/data/databases/virsorter-data\nVirSorter classifies its predictions into different confidence categories:\n\nCategory 1: “most confident” predictions\nCategory 2: “likely” predictions\nCategory 3: “possible” predictions\nCategories 4-6: predicted prophages\n\n\n\n\n\n\n\nStep\n\n\n\nWe then generate the corresponding viral sequence FASTA files using a custom python script parse_viral_pred.py as follows:\n    touch obs_results/virsorter_metadata.tsv\n    parse_viral_pred.py -a obs_results/ERR575691_host_filtered_filt500bp_renamed_filt1500bp.fasta -f obs_results/ERR575691_virfinder.txt  -s obs_results/virsorter_output/Predicted_viral_sequences/*.fasta -o obs_results\n\n\nFollowing the execution of this command, FASTA files (*.fna) will be generated for each one of the VIRify categories mentioned above containing the corresponding putative viral sequences.\nThe VIRify pipeline takes the output from VirFinder and VirSorter, reporting three prediction categories:\n\nHigh confidence: VirSorter phage predictions from categories 1 and 2.\nLow confidence:\nContigs that VirFinder reported with p-value &lt; 0.05 and score ≥ 0.9.\n\nContigs that VirFinder reported with p-value &lt; 0.05 and score ≥ 0.7, but that are also reported by VirSorter in category 3.\n\nProphages: VirSorter prophage predictions categories 4 and 5.\n\n\n\n\n\n\n\nStep\n\n\n\nWe will now restore the original contig names in the FASTA files (*fna) with using the mapping file mentioned earlier.\n    for file in $(find obs_results/ -maxdepth 1 -name '*fna' -type f); do BN=$(basename ${file} .fna); rename_fasta.py -i ${file} -m ERR575691_host_filtered_filt500bp_map.tsv -o obs_results/${BN}_original.fasta restore; done",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/virify.html#detection-of-viral-taxonomic-markers",
    "href": "sessions/virify.html#detection-of-viral-taxonomic-markers",
    "title": "Viral detection and classification",
    "section": "2. Detection of viral taxonomic markers",
    "text": "2. Detection of viral taxonomic markers\nOnce we have retrieved the putative viral sequences from the metagenomic assembly, the following step will be to analyse the proteins encoded in them in order to identify any viral taxonomic markers. To carry out this identification, we will employ a database of profile Hidden Markov Models (HMMs) built from proteins encoded in viral reference genomes. These profile HMMs were selected as viral taxonomic markers following a comprehensive random forest-based analysis carried out previously.\n\n\n\n\n\n\nStep\n\n\n\nThe VIRify pipeline uses prodigal for the detection of protein coding sequences (CDSs) and hmmscan for the alignment of the encoded proteins to each of the profile HMMs stored in the aforementioned database. This takes a while to run so we’ll copy over these results too.\n    cp exp_results/*.faa obs_results\n    cp exp_results/*modified.tsv obs_results/\n\n\n\n\n\n\n\n\nTip\n\n\n\nOnce the command execution finishes two new files will be stored for each category of viral predictions. The file with the suffix faa lists the proteins encoded in the CDSs reported by prodigal, whereas the file with the suffix modified.tsv contains all significant alignments between the encoded proteins and the profile HMMs, on a per-domain-hit basis.\n\n\n\n\n\n\n\n\nStep\n\n\n\nThe following command is used to parse the hmmer output and generate a new tabular file that lists alignment results in a per-query basis, which include the alignment ratio and absolute value of total E-value for each protein-profile HMM pair.\n    for file in $(find obs_results/ -name '*modified.tsv' -type f); do output_dir=\"$(dirname \"$file\")/$(basename \"$file\" .tsv)_informative.tsv\"; ratio_evalue_table.py -i ${file} -o ${output_dir} -t databases/additional_data_vpHMMs_v4.tsv; done",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/virify.html#viral-taxonomic-assignment",
    "href": "sessions/virify.html#viral-taxonomic-assignment",
    "title": "Viral detection and classification",
    "section": "3. Viral taxonomic assignment",
    "text": "3. Viral taxonomic assignment\nThe final output of the VIRify pipeline includes a series of gene maps generated for each putative viral sequence and a tabular file that reports the taxonomic lineage assigned to each viral contig. The gene maps provide a convenient way of visualizing the taxonomic annotations obtained for each putative viral contig and compare the annotation results with the corresponding assigned taxonomic lineage. Taxonomic lineage assignment is carried out from the highest taxonomic rank (genus) to the lowest (order), taking all the corresponding annotations and assessing whether the most commonly reported one passes a pre-defined assignment threshold.\n\n\n\n\n\n\nStep\n\n\n\nFirst, we are going to generate a tabular file that lists the taxonomic annotation results obtained for each protein from the putative viral contigs. We will generate this file for the putative viral sequences in each prediction category. Run the following:\n    for file in $(find obs_results/ -maxdepth 1 -name '*.faa' -type f); do viral_contigs_annotation.py -p ${file} -t ${file%prodigal.faa}modified_informative.tsv -o ${file%/*}; done\n\n\n\n\n\n\n\n\nStep\n\n\n\nNext, we will take the tabular annotation files generated and use them to create the viral contig gene maps:\n    for file in $(find obs_results/ -name '*annotation.tsv' -type f); do make_viral_contig_map.R -t ${file} -o ${file%/*}; done\n\n\n\n\n\n\n\n\nStep\n\n\n\nFinally, we will use the tabular annotation files again to carry out the taxonomic lineage assignment for each putative viral contig. Run the following command:\n    for file in $(find obs_results/ -name '*annotation.tsv' -type f); do contig_taxonomic_assign.py -i ${file} -o ${file%/*} -f databases/viphogs_cds_per_taxon_cummulative.csv -d databases/2022-11-01_ete3_ncbi_tax.sqlite; done\n\n\nFinal output results are stored in the obs_results/ directory.\nThe gene maps are stored per contig in individual PDF files (suffix names of the contigs indicate their level of confidence and category class obtained from VirSorter). Each protein coding sequence in the contig maps (PDFs) is coloured and labeled as high confidence (E-value &lt; 0.1), low confidence (E-value &gt; 0.1) or no hit, based on the matches to the HMM profiles. Do not confuse this with the high confidence or low confidence prediction of VIRify for the whole contig.\nTaxonomic annotation results per classification category are stored as text in the *taxonomy.tsv files.\n\n\n\n\n\n\nStep\n\n\n\nLet’s inspect the results. Do:\n    cat obs_results/*taxonomy.tsv\n\n\nYou should see a list of 9 contigs detected as viral and their taxonomic annotation in separate columns (partitioned by taxonomic rank). However, some do not have an annotation (e.g. NODE_4… and NODE_5…).\nNow on your computer on the left hand bar, select the folder icon.\nNavigate to Home –&gt; virify_tutorial –&gt; obs_results\nOpen the gene map PDF files of the corresponding contigs to understand why some contigs were not assigned to a taxonomic lineage. You will see that for these cases, either there were not enough genes matching the HMMs, or there was disagreement in their assignment.\n\n\n\nVirFinder scores",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/virify.html#running-the-practical-externally",
    "href": "sessions/virify.html#running-the-practical-externally",
    "title": "Viral detection and classification",
    "section": "Running the practical externally",
    "text": "Running the practical externally\nWe need to set up our computing environment in order to execute the commands above. Download the virify_tutorial_2023.tar.gz file containing all the data you will need using any of the following options:\n    wget http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2023/virify_tutorial_2023.tar.gz\n    #or\n    rsync -av --partial --progress rsync://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2023/virify_tutorial_2023.tar.gz .\nOnce downloaded, extract the files from the tarball:\n    tar -xzvf virify_tutorial_2023.tar.gz\nNow change into the virify_tutorial directory and setup the docker container by running the following commands in your terminal session:\n    cd virify_tutorial\n    docker pull quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    docker run --rm -it -v $(pwd):/opt/data quay.io/microbiome-informatics/2023-metagenomics-course-virify:1.0\n    mkdir obs_results\nThe container has the following tools installed: - Python - R - VirSorter - VirFinder\nAll scripts and databases used can be found in the data folder.\nYou can now start from section 1 above.",
    "crumbs": [
      "Home",
      "Sessions",
      "Viral detection and classification"
    ]
  },
  {
    "objectID": "sessions/mags.html",
    "href": "sessions/mags.html",
    "title": "MAG Generation",
    "section": "",
    "text": "Generation of metagenome assembled genomes (MAGs) from assemblies\nAssessment of quality\nTaxonomic assignment\n\n\n\nFor this tutorial, you will need to first start the docker container by running:\nsudo docker run --rm -it -v /home/training/Binning:/opt/data microbiomeinformatics/mgnify-ebi-2020-binning\npassword: training\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with checkM, and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to generate the input.fastq.sam.bam file yourself, you would run the following commands:\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step.\n\n# If you would like to practice this now, back up the input.fastq.sam.bam file that we provided first,\n# as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq ERR011322_2.fastq &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of bin.[0-9].fa.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?\n\n\nObviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data_2015_01_16.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nSTOP!!!! The following command fails. Please use backups instead, inspect the command visually only:\ncd /opt/data/assemblies\n# INSPECT BUT DO NOT RUN!\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**.\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nExample output of CheckM\n\n\nBefore we can visualize and plot the tree, we will need to convert the reference ID names used by CheckM to taxon names. We have already prepared a mapping file for renaming the tree (rename_list.tab). We can then do this easily with the newick utilities.\n\n\n\n\n\n\nStep\n\n\n\nTo do this, run the following command:\ncd /opt/data/\nnw_rename assemblies/checkm_output/storage/tree/concatenated.tre assemblies/rename_list.tab &gt; renamed.tree\n\n\n\n\n\n\nWe will now plot and visualize the tree we have produced. A quick and user-friendly way to do this is to use the web-based interactive Tree of Life iTOL\niTOL only takes in newick formatted trees, so we need to quickly reformat the tree with FigTree.\n\n\n\n\n\n\nStep\n\n\n\nIn order to open FigTree, open a new terminal window (without docker) and type figtree\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen the renamed.tree file with FigTree (File -&gt; Open) (file is in /home/training/Binning) and then select from the toolbar File -&gt; Export Trees. In the Tree file format select Newick and export the file as renamed.nwk (or choose a name you will recognise if you plan to use the shared account described below).\n\n\n\n\n\n\n\n\nStep\n\n\n\nTo use iTOL you will need a user account. For the purpose of this tutorial we have already created one for you with an example tree.\n\n\nGo to the iTOL website (open the link in a new window).\nThe login is as follows:\nUser: EBI_training\nPassword: EBI_training\nAfter you login, just click on My Trees in the toolbar at the top and select\nIBD_checkm.nwk from the Imported trees workspace.\nAlternatively, if you want to create your own account and plot the tree yourself follow these steps:\n\nAfter you have created and logged in to your account go to My Trees\nFrom there select Upload tree files and upload the tree you exported from FigTree\nOnce uploaded, click the tree name to visualize the plot\nTo colour the clades and the outside circle according to the phylum of each strain, drag and drop the files iTOL_clades.txt and iTOL_ocircles.txt present in /home/training/Data/Binning/iTOL_Files/ into the browser window\n\nOnce that is done, all the reference genomes used by CheckM will be coloured according to their phylum name, while all the other ones left blank correspond to the target genomes we placed in the tree. Highlighting each tip of the phylogeny will let you see the whole taxon/sample name. Feel free to play around with the plot.\nTo find the bins you generated, click on the search icon in the left-hand side menu (magnifying glass with the letters “Aa” in it). In the search field type bin.. Click on each bin name to see it in the tree.\n\n\n\n\n\n\nQuestion\n\n\n\nDoes the CheckM taxonomic classification make sense? Were you able to find all of the bins in the tree? If not, why do you think that is?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#prerequisites",
    "href": "sessions/mags.html#prerequisites",
    "title": "MAG Generation",
    "section": "",
    "text": "For this tutorial, you will need to first start the docker container by running:\nsudo docker run --rm -it -v /home/training/Binning:/opt/data microbiomeinformatics/mgnify-ebi-2020-binning\npassword: training",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "href": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "title": "MAG Generation",
    "section": "",
    "text": "Note\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with checkM, and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to generate the input.fastq.sam.bam file yourself, you would run the following commands:\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step.\n\n# If you would like to practice this now, back up the input.fastq.sam.bam file that we provided first,\n# as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq ERR011322_2.fastq &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of bin.[0-9].fa.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?\n\n\nObviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data_2015_01_16.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nSTOP!!!! The following command fails. Please use backups instead, inspect the command visually only:\ncd /opt/data/assemblies\n# INSPECT BUT DO NOT RUN!\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**.\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nExample output of CheckM\n\n\nBefore we can visualize and plot the tree, we will need to convert the reference ID names used by CheckM to taxon names. We have already prepared a mapping file for renaming the tree (rename_list.tab). We can then do this easily with the newick utilities.\n\n\n\n\n\n\nStep\n\n\n\nTo do this, run the following command:\ncd /opt/data/\nnw_rename assemblies/checkm_output/storage/tree/concatenated.tre assemblies/rename_list.tab &gt; renamed.tree",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#visualizing-the-phylogenetic-tree",
    "href": "sessions/mags.html#visualizing-the-phylogenetic-tree",
    "title": "MAG Generation",
    "section": "",
    "text": "We will now plot and visualize the tree we have produced. A quick and user-friendly way to do this is to use the web-based interactive Tree of Life iTOL\niTOL only takes in newick formatted trees, so we need to quickly reformat the tree with FigTree.\n\n\n\n\n\n\nStep\n\n\n\nIn order to open FigTree, open a new terminal window (without docker) and type figtree\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen the renamed.tree file with FigTree (File -&gt; Open) (file is in /home/training/Binning) and then select from the toolbar File -&gt; Export Trees. In the Tree file format select Newick and export the file as renamed.nwk (or choose a name you will recognise if you plan to use the shared account described below).\n\n\n\n\n\n\n\n\nStep\n\n\n\nTo use iTOL you will need a user account. For the purpose of this tutorial we have already created one for you with an example tree.\n\n\nGo to the iTOL website (open the link in a new window).\nThe login is as follows:\nUser: EBI_training\nPassword: EBI_training\nAfter you login, just click on My Trees in the toolbar at the top and select\nIBD_checkm.nwk from the Imported trees workspace.\nAlternatively, if you want to create your own account and plot the tree yourself follow these steps:\n\nAfter you have created and logged in to your account go to My Trees\nFrom there select Upload tree files and upload the tree you exported from FigTree\nOnce uploaded, click the tree name to visualize the plot\nTo colour the clades and the outside circle according to the phylum of each strain, drag and drop the files iTOL_clades.txt and iTOL_ocircles.txt present in /home/training/Data/Binning/iTOL_Files/ into the browser window\n\nOnce that is done, all the reference genomes used by CheckM will be coloured according to their phylum name, while all the other ones left blank correspond to the target genomes we placed in the tree. Highlighting each tip of the phylogeny will let you see the whole taxon/sample name. Feel free to play around with the plot.\nTo find the bins you generated, click on the search icon in the left-hand side menu (magnifying glass with the letters “Aa” in it). In the search field type bin.. Click on each bin name to see it in the tree.\n\n\n\n\n\n\nQuestion\n\n\n\nDoes the CheckM taxonomic classification make sense? Were you able to find all of the bins in the tree? If not, why do you think that is?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/assemblies.html",
    "href": "sessions/assemblies.html",
    "title": "Assembly and Co-assembly of Metagenomic Raw Reads",
    "section": "",
    "text": "Learning Objectives\nIn the following exercises you will learn how to perform metagenomic assembly and co-assembly, and to start exploring the output. We will shortly observe assembly graphs with Bandage, peek into assembly statistics with assembly_stats, and align contig files against the BLAST database.\n\n\n\n\n\n\nNote\n\n\n\nThe process of metagenomic assembly can take hours, if not days, to complete on a normal sample, as it often requires days of CPU time and 100s of GB of memory. In this practical, we will only investigate very simple example datasets.\n\n\nOnce you have quality filtered your sequencing reads, you may want to perform de novo assembly in addition to, or as an alternative to, read-based analyses. The first step is to assemble your sequences into contigs. There are many tools available for this, such as MetaVelvet, metaSPAdes, IDBA-UD, or MEGAHIT. We generally use metaSPAdes, as in most cases it yields the best contig size statistics (i.e. more continguous assembly), and it has been shown to be able to capture high degrees of community diversity (Vollmers, et al. PLOS One 2017). However, you should consider pros and cons of different assemblers, which not only includes the accuracy of the assembly, but also their computational overhead. Compare these factors to what you have available. For example, very diverse samples with a lot of sequence data (e.g. samples from the soil) uses a lot of memory with SPAdes. In the following practicals we will demonstrate the use of metaSPAdes on a small short-read sample, Flye on a long-read sample, and MEGAHIT to perform co-assembly.\n\n\nBefore we start…\nLet’s first move to the root working directory to run all analyses:\ncd /home/training/Assembly/\nYou will find all inputs needed for assemblies in the reads folder.\n\n\n\n\n\n\nNote\n\n\n\nIf anything goes wrong during the practical, you will find assembly backups for all steps in the respective “.bak” folders.\n\n\n\n\nShort-reads assemblies: metaSPAdes\nFor short reads, we will use SPAdes - St. Petersburg genome Assembler, a suite of assembling tools containing different assembly pipelines. For metagenomic data, we will the more metagenomi-specific side of SPAdes - metaSPAdes. metaSPAdes offers many options that fit your preferences differently, mostly depending on the type of data you are willing to assemble. To explore them, type metaspades.py -h. Bear in mind that options will differ when selecting different tools (e.g. spades.py) and they should be tuned according to the input dataset and desired outcome.\n\n\n\n\n\n\nTip\n\n\n\nThe default metaSPAdes pipeline executes an error correction step on the input fastqs. Since reads have already been polished in the past step, you can run metaspades without the error correction step.\nYou can see available metaspades parameters by typing the following:\nmetaspades.py -h \n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis execution should be able to run on a 4-core, 32 GB ram laptop. However, the command above will NOT work on your VMs to prevent overload on the system if all VM users are running this command at the same time. If you are willing to launch this, the execution will take up to 20 minutes.\n\n\nAn explanation of involved parameters follows: * -t 4 threads * --only-assembler skips the error correction step * -m 10 memory limit in Gb * -1 reads/oral_human_example_1_splitaa_trimmed_decontam.fastq forward read * -2 reads/oral_human_example_2_splitaa_trimmed_decontam.fastq reverse read *-o assembly_spades output folder\nOnce the assembly has completed, you will see plenty of files, in the assembly_spades folder including intermediate ones. contigs.fasta and scaffolds.fasta are the ones you are usually interested into for downstream analyses (e.g. binning and MAG generation). We will focus on contigs.fasta for this session, which is the same you are going to use in the coming practicals. Contigs in this file are ordered from the longest to the shortest. Without having to go all the way down to MAGs, you could sometimes identify strong taxonomic signals at the assembly stage with a quick blastn alignment.\n\n\n\n\n\n\nStep\n\n\n\nTake the first 100 lines of the sequence and perform a blast search (choose Nucleotide:Nucleotide from the set of options). Leave all other options as default on the search page. To select the first 100 lines of the assembly perform the following:\nhead -n 101 assembly.bak/contigs.fasta\nThe resulting output is going to look like this: \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat species does this sequence seem to be coming from?\nDoes this make sense as a human oral bacterium? Are you surprised by this result at all?\n\n\nAs mentioned in the theory talk, you might be interested in different statistics for your contigs file. assembly_stats is a tool that produces two simple tables in JSON format with various measures, including N10 to N50, GC content, longest contig length and more. The first section of the JSON corresponds to the scaffolds in the assembly, while the second corresponds to the contigs.\nN50 is a measure to describe the quality of assembled genomes that are fragmented in contigs of different length, used to assess the sequence length of the shortest contig at 50% of the total assembly length (after sorting assembled contigs from longest to shortest).\nA (hopefully) clarifying picture to understand N50: \nEssentially, the higher this value, the better, as it means that longer (i.e. more complete) contigs cover a certain fraction of the final assembly. However, this only makes sense when thinking about alike metagenomes. Note that, like N50, other values can be considered e.g. N90 is the shortest contig length to cover 90 percent of the metagenome.\n\n\n\n\n\n\nStep\n\n\n\nassembly-stats assembly.bak/scaffolds.fasta\n\n\nYou will see a short output with a few statistics for your assembly. In lines with format N50 = YYY, n = Z, n represents the amount of sequences needed to cover 50% of the total assembly. A “gap” is any consecutive run of Ns (undetermined nucleotide bases) of any length. N_count is the total Ns (undetermined nucleotide bases) across the entire assembly.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the length of the longest and the shortest contigs?\nWhat is the N50 of the assembly? Given that input sequences were ~150bp long paired-end sequences, what does this tell you about the assembly?\n\n\nAnother tool to keep in mind for metagenomic assemblies is QUAST, which provides a deeper insight on assemblies statistics like indels and misassemblies rate in a very short time.\n\n\nLong-reads assemblies: Flye\nFor long-reads, we will use Flye, which assembles single-molecule sequencing reads like PacBio and Oxford Nanopore Technologies (ONT) reads. As spades, Flye is a pipeline that takes care of assembly polishing. Similarly to assembly scaffolding, it tries to overcome long-reads base call error by comparing different reads that cover the same sequencing fragment. Flye’s parameters are quickly described in the help command (flye -h).\nFlye supports metagenomic assemblies with the --meta flag. Backup assemblies for this section can be found in the Assembly folder, starting with “LR”.\n\n\n\n\n\n\nStep\n\n\n\nmkdir flye_out\nflye --nano-raw reads/ONT_example.fastq --out-dir flye_out --threads 4\n\n\n\n\n\n\n\n\nExtra\n\n\n\nNote that we are not using the --meta flag. If you have some spare time, try to execute the same command with this flag and output folder “LR_meta_assembly”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEach execution will take around 5 minutes.\n\n\nLet’s have a first look at how assembly graphs look like. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a program that creates interactive visualisations of assembly graphs. They can be useful for finding sections of the graph, such as rRNA, or identify specific parts of a genome. Note, you can install Bandage on your local system. With Bandage, you can zoom and pan around the graph and search for sequences, and much more.\nWhen looking at metaSPAdes output, it is usually recommended to launch Bandage on assembly_graph.fastg. However, our assembly is quite fragmented, so we will load assembly_graph_after_simplification.gfa.\nWe will use Bandage to compare the two assemblies we have generated, Flye and metaSPAdes.\n\n\n\n\n\n\nStep\n\n\n\nBandage\nIn the Bandage GUI perform the following:\n\nSelect File -&gt; Load graph\nNavigate to Home/training/Data/Assembly/assembly.bak and open assembly_graph_after_simplification.gfa\n\nOnce loaded, you need to draw the graph. To do so, under the “Graph drawing” panel on the left side perform the following:\n\nSet Scope to Entire graph\nClick on Draw graph\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you find any large, complex parts of the metaSPAdes graph? If so, what do they look like?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the Flye assembly differ from the one generated with metaSPAdes?\n\n\n\n\n\n\n\n\nExtra\n\n\n\nWe launched Flye both with and without --meta on file reads/ONT_example.fastq. This file actually comes from run ERR3775163, which can be browsed on ENA. Have a look at sample metadata. Despite dealing with a long-read sample, can you understand why the assembly graph looks better for the execution without the --meta option?\n\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you blast the first contig of the long-read assembly, do results match the metadata you find on ENA?\n\n\n\n\nCo-assemblies: MEGAHIT\nIn the following steps of this exercise, we will perform co-assembly of multiple datasets. The first execution requires around 6-7 minutes to finish, the general suggestion is to run the first instruction and then rely on files in the coassembly.bak directory, which contains all expected results.\n\n\n\n\n\n\nStep\n\n\n\nFirst, make sure that output directories do not already exist, as MEGAHIT can’t overwrite existing directories. Bear in mind that backup assemblies for this exercise are in:\nls /home/training/Assembly/coassembly.bak\nYou can perform co-assemblies with MEGAHIT as follows:\nmegahit -1 reads/oral_human_example_1_splitaa_kneaddata_paired_1.fastq -2 reads/oral_human_example_1_splitaa_kneaddata_paired_2.fastq -o coassembly1_new -t 4 --k-list 23,51,77 \n\nmegahit -1 reads/oral_human_example_1_splitaa_kneaddata_paired_1.fastq,reads/oral_human_example_1_splitab_kneaddata_paired_1.fastq -2 reads/oral_human_example_1_splitaa_kneaddata_paired_2.fastq,reads/oral_human_example_1_splitab_kneaddata_paired_2.fastq -o coassembly2_new -t 4 --k-list 23,51,77 \n\nmegahit -1 reads/oral_human_example_1_splitaa_kneaddata_paired_1.fastq,reads/oral_human_example_1_splitab_kneaddata_paired_1.fastq,reads/oral_human_example_1_splitac_kneaddata_paired_1.fastq -2 reads/oral_human_example_1_splitaa_kneaddata_paired_2.fastq,reads/oral_human_example_1_splitab_kneaddata_paired_2.fastq,reads/oral_human_example_1_splitac_kneaddata_paired_2.fastq -o coassembly3_new -t 4 --k-list 23,51,77\n\n\nYou should now have three different co-assemblies generated from different subsamples of the same data.\n\n\n\n\n\n\nStep\n\n\n\nCompare the results of the three contig files with assembly_stats. How do these assemblies differ from the one generated previously with metaSPAdes? Which one do you think is best?\nTo generate assembly graphs for MEGAHIT output, execute:\nmegahit_toolkit contig2fastg 77 final.contigs.fa &gt; final.contigs.fastg\n\n\n\n\n… And now?\nIf you have reached the end of the practical and have some spare time, look at the paragraphs labelled “Extra”. They contain optional exercises for the curious student :)\n\n\n…….. Yes, but now that I am really, really done?\nYou could try to assemble raw reads with different assemblers or parameters, and compare statistics and assembly graphs. Note, for example, that metaSPAdes can deal ONT data (but it will likely yield a lower quality assembly).\n\n\n\n\nReuseApache 2.0",
    "crumbs": [
      "Home",
      "Sessions",
      "Assembly and Co-assembly of Metagenomic Raw Reads"
    ]
  },
  {
    "objectID": "sessions/qc.html",
    "href": "sessions/qc.html",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/qc_results\n\n\nFor this tutorial, you’ll need to move into the working directory and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/quality\nchmod -R 777 /home/training/quality\nexport DATADIR=/home/training/quality\nchmod -R 777 /home/training/qc_results\nexport RESDIR=/home/training/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host” Now start the Docker container:\ndocker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY microbiomeinformatics/biata-qc-assembly:v2021",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#prerequisites",
    "href": "sessions/qc.html#prerequisites",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/qc_results\n\n\nFor this tutorial, you’ll need to move into the working directory and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/quality\nchmod -R 777 /home/training/quality\nexport DATADIR=/home/training/quality\nchmod -R 777 /home/training/qc_results\nexport RESDIR=/home/training/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host” Now start the Docker container:\ndocker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY microbiomeinformatics/biata-qc-assembly:v2021",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "href": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Quality control and filtering of the raw sequencing read files",
    "text": "Quality control and filtering of the raw sequencing read files\n\n\n\n\n\n\nLearning Objectives\n\n\n\nIn the following exercises, you’ll learn how to check the quality of short read sequences, identify adaptor sequences, remove adapters and low-quality sequences, and construct a reference database for host decontamination.\n\n\n\n\n\n\n\n\nHere you should see the contents of the working directory.\n\n\n\nThese are the files we’ll use for the practical:\nls /opt/data\n\n\n\nQuality assessment with FastQC and multiqc\nWe will start by using a tool called FastQC, which will generate a report describing multiple quality measures for the given reads.\n\n\n\n\n\n\nGenerate a directory of the FastQC results\n\n\n\nmkdir -p /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_1_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_2_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\nThe -o option used with FastQC sends the output files to the given path.\n\n\n\n\n\n\nNow on your computer, select the folder icon.\n\n\n\nNavigate to Home → quality → fastqc_results\nRight-click on file oral_human_example_1_splitaa_fastqc.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of FastQC\n\n\nSpend some time looking at the ‘Per base sequence quality.’\n\n\nFor each position, a BoxWhisker-type plot is drawn:\n\nThe central red line is the median value.\nThe yellow box represents the inter-quartile range (25-75%).\nThe upper and lower whiskers represent the 10% and 90% points.\nThe blue line represents the mean quality.\n\nThe y-axis on the graph shows the quality scores. The higher the score, the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it’s common to see base calls falling into the orange area towards the end of a read.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this tell you about your sequence data? When do the errors start?\n\n\nIn the pre-processed files, we see two warnings, as shown on the left side of the report. Navigate to the “Per bases sequence content.”\n\n\n\nScreenshot of FastQC\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAt around 15-19 nucleotides, the DNA composition becomes very even; however, at the 5’ end of the sequence, there are distinct differences. Why do you think that is?\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen up the FastQC report corresponding to the reversed reads.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any significant differences between the forward and reverse files?\n\n\nFor more information on the FastQC report, please consult the ‘Documentation’ available from this site: FastQC Documentation\nWe are currently only looking at two files, but often we want to look at many files. The tool multiqc aggregates the FastQC results across many samples and creates a single report for easy comparison. Here we will demonstrate the use of this tool.\n\n\n\n\n\n\nRun\n\n\n\nmkdir /opt/results/multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/multiqc_results/oral\nchown 1001 /opt/results/multiqc_results/oral/*.html\n\n\nIn this case, we provide the folder containing the FastQC results to multiqc, and similar to FastQC, the -o argument allows us to set the output directory for this summarized report.\n\n\n\n\n\n\nNow on your computer, select the folder icon.\n\n\n\nNavigate to Home → quality → multiqc_results\nRight-click on file multiqc_report.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as two separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat fraction of reads are duplicates?\n\n\n\n\nQuality filtering with fastp\nSo far we have looked at the raw files and assessed their content, but we have not done anything about removing duplicates, sequences with low quality scores, or removal of the adaptors. So, let’s start this process.\nOur first step will be to perform quality filtering of the reads using a tool called fastp, which is versatile, easy to use, and fast.\n\n\n\n\n\n\nCreate directories that will store output files from the cleaning process\n\n\n\nmkdir -p /opt/results/cleaned/oral\n\n\nThe fastp command you will run contains multiple parameters, so let’s slowly deconstruct it:\n\n\n\n\n\n\nRun\n\n\n\nfastp --in1 /opt/data/oral_human_example_1_splitaa.fastq.gz \\\n      --in2 /opt/data/oral_human_example_2_splitaa.fastq.gz \\\n      --out1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/oral/oral.fastp.json --html /opt/results/cleaned/oral/oral.fastp.html\n\n\n\n--in1/--in2 — The two input paired-end read files\n--out1/--out2 — The two output files after filtering\n-l — Minimum read length required, reads below 50 in this case are discarded\n--cut_right/--cut_right_window_size/--cut_right_mean_quality — These three options all work together. --cut_right creates a window, of size specified by --cut_right_window_size, which will slide from the front to the tail of the reads, calculating the mean quality score in the window at every step. If at any point, the mean quality score value is lower than the one specified by --cut_right_mean_quality, then the bases of that window and everyting to its right are immediately discarded for that read.\n-t 1 — Will trim the tail of its final base, as it’s a lot lower quality than other positions. This is a setting you should set very purposefully and for good reason, like we’re doing here.\n--detect_adapter_for_pe — One of the very useful features of fastp is that it can detect adapters automatically and remove them, which this parameter activates.\n--json/--html — Outputs a summary report similar to FastQC, in both .json and .html formats.\n\n\n\n\n\n\n\nQuestion\n\n\n\nFind and open the .html report. How many reads were removed? How has the average quality of the reads changed? \n\n\n\n\nDecontamination with bowtie2\nNext, we want to remove any potential contamination in our reads, which we’ll do using a tool called bowtie2. It is always good to routinely screen for human DNA (which may come from the host and/or staff performing the experiment). However, if the sample is from a mouse, you would want to download the mouse genome. The first step in the decontamination process is therefore to make a database that our reads will be searched against for sources of contamination.\nIn the following exercise, we are going to use two “genomes” already downloaded for you in the decontamination folder. To make this tutorial quicker and smaller in terms of file sizes, we are going to use PhiX (a common spike-in) and just chromosome 10 from human.\n\n\n\n\n\n\nThe reference sequences files we’ll be using\n\n\n\nls /opt/data/decontamination\n\n# Output: GRCh38_chr10.fasta  phix.fasta\n\n\nFor the next step, we need one file, so we want to merge the two different fasta files. This is simply done using the command-line tool cat.\n\n\n\n\n\n\nRun\n\n\n\ncat /opt/data/decontamination/GRCh38_chr10.fasta /opt/data/decontamination/phix.fasta &gt; /opt/data/decontamination/chr10_phix.fasta\n\n\nYou will often need to build indices for large sequence files - including sequencing files and reference files - to speed up computation. To build a bowtie index for our new concatenated PhiX-chr10 file:\n\n\n\n\n\n\nRun\n\n\n\nbowtie2-build /opt/data/decontamination/chr10_phix.fasta /opt/data/decontamination/chr10_phix.index\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible to automatically download a pre-indexed human genome in bowtie2\n\n\nNow we are going to use our new indexed chr10_phix reference and decontaminate our already quality-filtered reads from fastp. Using bowtie2 again:\n\n\n\n\n\n\nRun\n\n\n\nbowtie2 -1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/oral/oral_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the bowtie2 output on the terminal, what fraction of reads have been deemed to be contaminating?\n\n\nbowtie2 changes the naming scheme of the output files, so we rename them to be consistent:\n\n\n\n\n\n\nRun\n\n\n\nmv /opt/results/cleaned/oral/oral_human_example.fastq.1.gz /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/oral/oral_human_example.fastq.2.gz /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz\n\n\n\n\nPost-QC assessment with FastQC and multiqc\n\n\n\n\n\n\nRun FastQC\n\n\n\nUsing what you have learned previously, generate a FastQC report for each of the *trimmed_decontam.fastq.gz files. Output the new fastqc report files in the same /opt/results/fastqc_results/oral directory as last time.\n\n\n\n\n\n\n\n\nRun multiQC\n\n\n\nAlso generate a multiQC report, with /opt/results/fastqc_results/oral as input. The reason we generated the new FastQC reports in the same directory is so that you can compare how the reads have changed after the quality filtering and decontamination steps in the same final multiqc report.\nmkdir -p /opt/results/final_multiqc_results/oral\n&lt;you construct the command&gt;\n\n\n\n\n\n\n\n\nCheck report\n\n\n\nView the MultiQC report as before using your browser.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as four separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of the change in sequence quality histograms? Have they improved?\nDid sequences at the 5’ end become uniform? Why might that be? Is there anything that suggests that adaptor sequences were found?\n\n\nThe reads have now been decontaminated and can be uploaded to ENA, one of the INSDC members. It is beyond the scope of this course to include a tutorial on how to submit to ENA, but there is additional information available on how to do this in this Online Training guide provided by EMBL-EBI",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#assembly-phix-decontamination",
    "href": "sessions/qc.html#assembly-phix-decontamination",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Assembly PhiX decontamination",
    "text": "Assembly PhiX decontamination\n\n\n\n\n\n\nLearning Objectives\n\n\n\nIn the following exercises, you will generate a PhiX BLAST database and run a BLAST search with a subset of assembled freshwater sediment metagenomic reads to identify contamination.\n\n\nPhiX, used in the previous section of this practical, is a small bacteriophage genome typically used as a calibration control in sequencing runs. Most library preparations will use PhiX at low concentrations; however, it can still appear in the sequencing run. If not filtered out, PhiX can form small spurious contigs that could be incorrectly classified as diversity.\n\n\n\n\n\n\nGenerate the PhiX reference BLAST database:\n\n\n\nmakeblastdb -in /opt/data/decontamination/phix.fasta -input_type fasta -dbtype nucl -parse_seqids -out /opt/data/decontamination/phix_blastDB\n\n\nPrepare the freshwater sediment example assembly file and search against the new BLAST database. This assembly file contains only a subset of the contigs for the purpose of this practical.\n\n\n\n\n\n\nRun\n\n\n\nmkdir -p /opt/results/blast_results\ngunzip /opt/data/freshwater_sediment_contigs.fa.gz\nblastn -query /opt/data/freshwater_sediment_contigs.fa -db /opt/data/decontamination/phix_blastDB -task megablast -word_size 28 -best_hit_overhang 0.1 -best_hit_score_edge 0.1 -dust yes -evalue 0.0001 -min_raw_gapped_score 100 -penalty -5 -soft_masking true -window_size 100 -outfmt 6 -out /opt/results/blast_results/freshwater_blast_out.txt\n\n\nThe BLAST options are:\n\n-query — Input assembly fasta file.\n-out — Output file\n-db — Path to BLAST database.\n-task — Search type -“megablast”, for very similar sequences (e.g, sequencing errors)\n-word_size — Length of initial exact match\n\n\n\n\n\n\n\nAdd headers to the blast output and look at the contents of the final output file:\n\n\n\ncat /opt/data/blast_outfmt6.txt /opt/results/blast_results/freshwater_blast_out.txt &gt; /opt/results/blast_results/freshwater_blast_out_headers.txt\ncat /opt/results/blast_results/freshwater_blast_out_headers.txt\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre the hits significant?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the matching contigs? We would typically filter metagenomic contigs at a length of 500bp. Would any PhiX contamination remain after this filter?\n\n\nNow that PhiX contamination was identified, it is important to remove these contigs from the assembly file before further analysis or upload to public archives. Just like you learnde in the last section, you could use a tool like bowtie2 to achieve this.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#using-negative-controls",
    "href": "sessions/qc.html#using-negative-controls",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Using Negative Controls",
    "text": "Using Negative Controls\n\n\n\n\n\n\nLearning Objectives\n\n\n\nThis exercise will look at the analysis of negative controls. You will assess the microbial diversity between a negative control and a skin sample.\n\n\nThe images below show the taxonomic classification of two samples: a reagent negative control and a skin metagenomic sample. The skin sample is taken from the antecubital fossa - the elbow crease, which is moist and a site of high microbial diversity. The classification was performed using a tool called Kraken2. Kraken2 takes a while to run, so we have done this for you and plotted the results. An example of the command used to do this. DO NOT run this now:\n\nkraken2 --db standard_db --threshold 0.10 --threads 8 --use-names --fastq-input --report out.report --gzip-compressed in_1.fastq.gz in_2.fastq.gz See the kraken2 manual for more information\n\nSee Pavian manual for the plots.\nThe following image shows the microbial abundance in the negative control: \nThe following image shows the microbial abundance in the skin sample: \n\n\n\n\n\n\nStep\n\n\n\nLook for similarities and differences at both the phylum and genus level - labelled as ‘P’ and ‘G’ on the bottom axis.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any overlap between the negative control and skin sample phylum? Can we map the negative control directly to the skin sample to remove all contaminants? If not, why?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any genera in the negative control which aren’t present in the skin sample? If you do a google search of this genus, where are they commonly found? With this information, where could this bacteria in the negative control have originated from?\n\n\nIf you have finished the practical you can try this step for more practice assessing and trimming datasets, there is another set of raw reads called “skin_example_aa” from the skin metagenome available. These will require a FastQC or multiqc report, followed by quality filtering and mapping to the reference database with fastp and bowtie2. Using what you have learned previously, construct the relevant commands. Remember to check the quality before and after trimming.\n\n\n\n\n\n\nNavigate to skin folder and run quality control.\n\n\n\nls /opt/data/skin\n# Output: skin_example_aa_1.fastq.gz  skin_example_aa_2.fastq.gz  skin_neg_control.fastq.gz\n\n\nRemember you will need to run the following command to view any html files in the VM browsers:\nchown 1001 foldername/*.html",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#running-the-practical-externally",
    "href": "sessions/qc.html#running-the-practical-externally",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Running the practical externally",
    "text": "Running the practical externally\nWe need to first fetch the practical datasets.\n    wget http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz\n    or\n    rsync -av --partial --progress rsync://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz .\nOnce downloaded, extract the files from the tarball:\n    tar -xzvf quality.tar.gz\nWe also need the trimmomatic binary\n    cd quality\n    wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip\n    unzip Trimmomatic-0.39.zip\n    cd ..\nNow pull the docker container and set the above quality directory as DATADIR.\n    docker pull microbiomeinformatics/biata-qc-assembly:v2021\n    export DATADIR={path to quality directory}\n    xhost +\nYou will see the message “access control disabled, clients can connect from any host”\n    docker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY microbiomeinformatics/biata-qc-assembly:v2021\nThe container has the following tools installed:\n\nfastqc\nmultiqc\nfastp\nbowtie2\nblast\n\nYou can now continue this practical from the section “Quality control and filtering of the raw sequence files”",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html",
    "href": "sessions/mag_catalogues.html",
    "title": "MAG Catalogues",
    "section": "",
    "text": "MAGs 1 are an approach to deriving genome-resolved information from metagenomic datasets.\nMGnify’s MAG Catalogues are biome-specific, clustered, annotated collections of MAGs. Biomes are selected on the grounds of data availability, community interest, and project objectives.\n\n\n\n\n\n\n\n\nSearch the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.\n\n\n\n\n\nSourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5\n\n\n\n\n\nThe MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "href": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "title": "MAG Catalogues",
    "section": "",
    "text": "Search the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "href": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "title": "MAG Catalogues",
    "section": "",
    "text": "Sourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "href": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "title": "MAG Catalogues",
    "section": "",
    "text": "The MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#footnotes",
    "href": "sessions/mag_catalogues.html#footnotes",
    "title": "MAG Catalogues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMetagenome Assembled Genomes↩︎\nHint… what does anthropi in the species J. anthropi derive from?↩︎\nHint… each MAG’s detail page overview tab shows stats including completeness, contamination, and N50.↩︎\nIf you got lost earlier, download it from MGYG000304175.fna↩︎\nThere are interesting use cases for researchers (checking which environments a species is found in, checking whether a newly assembled genome is novel etc), as well as use cases for services like MGnify (cross-linking genomes between catalogues where those datasets are not clustered together).↩︎\nApplication Programming Interface↩︎",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/pangenome.html",
    "href": "sessions/pangenome.html",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals should be available on the virtual machine. Alternatively, you can download all of the files from google drive:\nhttps://drive.google.com/drive/folders/1pc17JErBHRwLeRf8Ch8yu4dXSIwFfBQK?usp=share_link\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/course_dir/data_dir/MGnify_training_course/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#preamble",
    "href": "sessions/pangenome.html#preamble",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals should be available on the virtual machine. Alternatively, you can download all of the files from google drive:\nhttps://drive.google.com/drive/folders/1pc17JErBHRwLeRf8Ch8yu4dXSIwFfBQK?usp=share_link\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/course_dir/data_dir/MGnify_training_course/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#building-poppunk-models",
    "href": "sessions/pangenome.html#building-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "1. Building PopPUNK models",
    "text": "1. Building PopPUNK models\nWe will be using 112 B. uniformis isolate genomes (i.e. not MAG data). We are going to use these to define subspecies within the population using PopPUNK. These can be listed using:\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model\nInstallation instructions and an overview are available.\nPopPUNK is already installed on the VM. Confirm this by running poppunk --version.\n\nCreating the database\nThe first step to running PopPUNK on a new species is to create a ‘database’ which contains sketches of the genomes and calculates all of the core and accessory distances between the samples. We will be following the guidance in the relevant section of the documentation.\nFirst, navigate to the working directory and create a new directory:\ncd ~/course_dir/work_dir/Day_5 && mkdir pangenome && cd pangenome\nWe need to create a list of the input files. This needs to have the sample names and the location of files with their genomes. This can be created in many ways, here we will use a simple bash command:\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model/MGYG*) &gt; rfile.txt\nWe can now create the input database with --create-db as follows:\npoppunk --create-db --output b_uniformis_db --r-files rfile.txt --min-k 17 --max-k 33 --plot-fit 10 --threads 4\nThis will run on 4 CPU cores to increase the speed. The other option is the range of k-mer lengths used, see the documentation for more information. Have a look at the plots in the output directory e.g. b_uniformis_db/b_uniformis_db_distanceDistribution.png and some of the fit examples such as b_uniformis_db_fit_example_1.pdf.\nUsually we want to run some quality control on the data, which we can do as follows:\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.05 --max-a-dist 1 --output b_uniformis_db_qc\nThis removes outlying distances and poor quality samples. In this case the data is all good quality and should all be retained.\nHowever if we run with a smaller core (pi) distance, this will remove half of the samples.\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.01 --max-a-dist 1 --output b_uniformis_db_qc\nThis isn’t a good idea here as this core distance is too strict for a species as diverse as B. uniformis.\n\n\nFitting a model\nWe now need to create a ‘model’ which determines a cutoff below which genomes are considered the same subspecies.\nThere are many options available, as detailed in the documentation.\nAs we have ‘small’ sample collection with strain-structure where distance distribution components are clearly separated, we’ll try the Bayesian Gaussian Mixture Model with two components:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K2 --K 2\nFrom the output to the console we can see that everything is in one cluster (In Network Summary there is one component) and so we haven’t found any subspecies. Have a look at the output plot b_uniformis_BGMM_K2/b_uniformis_BGMM_K2_DPGMM_fit.png too.\nIt looks like adding an extra component (‘blob’) might help, so let’s try that and make three rather than two:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K3 --K 3\nThat looks better and there are some clusters (In Network Summary there are is seven components), but if you look at the same plot again (b_uniformis_BGMM_K3/b_uniformis_BGMM_K3_DPGMM_fit.png) it doesn’t look like a great fit to the data.\nIn this case the data is actually a bit sparse to automatically get a good fit and we can do a much better job if we enrich the dataset with a few thousand MAGs and then use the ‘refine’ model mode. But for now we’ll take a shortcut and impose a cutoff that looks like it will work by using the threshold model:\npoppunk --fit-model threshold --ref-db b_uniformis_db --output b_uniformis_threshold --threshold 0.0025\nIn fact, this way of choosing a model is supported by population genetic simulations (see this paper). So we are acutally free to choose a cutoff that defines subspecies which ‘work’ for us, and the models in PopPUNK are mostly a more convenient way of automating this to find useful clusters in general.\nAs you can see, this is all a bit fiddly and requires iteration. It is usually better to use a previously fitted and tested model, which we will cover in the next part.\n\n\nVisualising the results\nBut before we move on, let’s get a better look at the results. We can make a core genome tree and accessory genome embedding and plot the clusters interactively in the browser. First, run the poppunk_visualise command.\npoppunk_visualise --ref-db b_uniformis_db --model-dir b_uniformis_threshold --microreact --maxIter 100000 --output b_uniformis_db_viz\nHere, maxIter is being used to reduce the number embedding iterations as the dataset is small, just so the command runs quickly.\nNow, open up https://microreact.org/ in your browser and choose ‘Upload’. Drag and drop the .microreact file in the b_uniformis_db_viz directory to see the clusters and the tree. Do they look ok?\n\n\nFurther analysis\nIf you have time, try playing around with different methods of fitting PopPUNK models in the documentation.\nYou can experiment with HDBSCAN, a method for automatically detecting the number and position of clusters.\nOnce you have a model fitted using BGMM or HDBSCAN, you can also refine it. This method takes a previously identified within-strain boundary and moves it to optimise the network score of the strain cluster network. We always recommend refining a previous fit, as it may significantly improve strain assignments.\nYou can also play around with visualisation tools such as Cytoscape. Use the poppunk_visualise tool to generate a cytoscape output following the documentation. This will allow you to visualise the strain cluster network, the components of which are the strains detected by PopPUNK. Note: this will generate a file for each component, as well as the whole network, enabling visualisation of the whole network or just parts of it.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#assigning-using-poppunk-models",
    "href": "sessions/pangenome.html#assigning-using-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "2. Assigning using PopPUNK models",
    "text": "2. Assigning using PopPUNK models\nIt is faster to ‘assign’ new samples to an existing definition of subspecies. This has the bonus that their names will be consistent across studies.\nTypically, you can download an existing database with a fit from https://www.bacpop.org/poppunk/. If you have a fit for a new species please send it to us and we can share it here.\nThere is no fit for B. uniformis (yet…) so we’ll use the one we just made.\n\nUsing poppunk_assign to assign MAGs to subspecies\nNow we’ll work with a large collection of MAGs. These are the B. uniformis MAGs from MGnify with &gt;95% completeness and &lt;5% contamination. They can be listed here\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta/\nPopPUNK distances are relatively robust to missing sequence content seen in MAGs, but less able to deal with contamination.\nAgain, navigate to the work directory and create the list of input files for poppunk:\ncd ~/course_dir/work_dir/Day_5/pangenome\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta/*.fasta) &gt; qfile.txt\nOne small problem is that these MAGs also contain the isolates from before. PopPUNK will refuse to assign these without unique names. Here’s a bash incantation to remove the duplicates:\ncut -f 1 rfile.txt &gt; ref_names.txt && grep -v -w -f ref_names.txt qfile.txt &gt; qfile_nodups.txt\nmv qfile_nodups.txt qfile.txt\nThe command is relatively simple, we need to provide the database, the model directory and the input ‘query’ genomes to assign. Quality control is ‘built-in’:\npoppunk_assign --db b_uniformis_db --model-dir b_uniformis_threshold --query qfile.txt --output b_uniformis_queries --threads 4 --max-merge 3 --run-qc --max-a-dist 0.8\nThe b_uniformis_queries_clusters.csv file contains the subspecies assignments.\nThe visualisation command is a bit more involved as we need to point to both directories and the model directory:\npoppunk_visualise --ref-db b_uniformis_db --query-db b_uniformis_queries --model-dir b_uniformis_threshold --microreact --output b_uniformis_query_viz --threads 4 --maxIter 10000000 --previous-query-clustering b_uniformis_queries/b_uniformis_queries_clusters.csv --previous-clustering b_uniformis_threshold/b_uniformis_threshold_clusters.csv\nLoad the .microreact output file in the b_uniformis_query_viz directory in Microreact again to see the output.\nUse the menu under the ‘eye’ to change from reference/query colour (‘Status’) to ‘Cluster_Cluster’ to see the clusters.\n\n\nUpdating the database\nYou’ll see that some MAGs formed new clusters (‘novel clusters’), whilst others were merged, as the new genomes connected to multiple clusters generating in the original database.\nIt is possible to permanently add the query genomes to the database, so future uses make use of novel cluster assignments. Simply add --update-db to the command above. This is beyond the scope of this practical but is documented here and here.\n\n\nFurther analysis\nIf you have time, try generating another visualisation of the query network using Cytoscape. How does this network compare to the original one from the previous section, when you only built using isolate genomes and not MAG data?\nAlso, try building a PopPUNK model as before, but only using the MAG data. How do the distance distributions compare between the isolate data and MAG data? Does a threshold model work with the data, or would an automated method for model fitting work better?\nOnce you have a model fitted using BGMM or HDBSCAN, you can also refine it. This method takes a previously identified within-strain boundary and moves it to optimise the network score of the strain cluster network. We always recommend refining a previous fit, as it may significantly improve strain assignments.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "href": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "title": "Pangenome analysis of metagenomic data",
    "section": "3. Finding core and accessory genes",
    "text": "3. Finding core and accessory genes\nWe can use a probabilistic model to correct for the fact that we know MAGs are missing some genes. If we don’t do this then we will systematically under-estimate their population frequency, and end up with nothing in the core genome.\n\nCELEBRIMBOR: a pipeline for pangenome analysis and threshold correction\nThe CELEBRIMBOR prerequisities are already install on the VM. The source code files can be found in the CELEBRIMBOR directory which can be listed here :\nls /home/training/course_dir/data_dir/MGnify_training_course/CELEBRIMBOR\nAs well be editing the source code, make a copy in your working directory :\ncp -r /home/training/course_dir/data_dir/MGnify_training_course/CELEBRIMBOR ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nThe files we will be analysing can be listed here :\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR\nThis archive contains two directories. CELEBRIMBOR_MAGs contains the fasta files we will be analysing. results contains the Bakta and CheckM outputs which were generated previously from these genomes. As these are the slowest parts of the analysis, we have provided them to allow you to generate results faster.\nAs we’ll be updating results, copy the whole directory to your working directory :\ncp -r ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/results ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nCELEBRIMBOR is a snakemake pipeline which enables automated gene annotation, clustering, completeness estimation and core threshold adjustment. Snakemake allows workflows to be re-run if workflows stop prematurely.\nTo prevent CELEBRIMBOR from re-running Bakta and CheckM, we must update the creation times of the input and output files using touch. If you do your own analysis outside of this practical, this is not recommended as you would want to run the full CELEBRIMBOR pipeline.\n(cd ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/CELEBRIMBOR_MAGs && touch *.fasta)\n(cd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && touch annotated/*/* && touch annotated/*)\n(cd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && touch checkm_out/* && touch checkm_out/*/* && && touch checkm_out/*/*/*)\n(cd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && touch checkm_out.tsv)\nDocumentation for CELEBRIMBOR can be found here.\nSnakemake reads a config.yaml file to assign parameters. Navigate to the CELEBRIMBOR directory, and update the config.yaml file with the appropriate parameters.\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nvim config.yaml\nTo start typing in vim, type i. To close vim, first use the esc key, and then WITH saving type, :wq, or WITHOUT saving, type :q!\n#output directory\noutput_dir: /home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\n\n# dir with genome fasta files, must have '.fasta' extension, to convert use 'rename .fa .fasta  *.fa' (e.g. if extensions are .fa)\ngenome_fasta: /home/training/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/CELEBRIMBOR_MAGs\n\n# path to bakta DB:\nbakta_db: /home/training/course_dir/data_dir/db-light\n\n# cgt executable parameters\ncgt_exe: /home/training/.cargo/bin/cgt_bacpop\ncgt_breaks: 0.05,0.95\ncgt_error: 0.05\n\n# choice of clustering method, either \"mmseqs2\" or \"panaroo\"\nclustering_method: \"mmseqs2\"\n\n# must be one of \"strict\", \"moderate\" or \"sensitive\"\npanaroo_stringency: \"strict\"\nNote: paths in the config.yaml file cannot contain ~ symbols.\nAs CELEBRIMBOR runs CheckM, we’ll need to point to the correct database :\ncheckm data setRoot ~/course_dir/data_dir/MGnify_training_course/pangenome_mgnify_env/checkm_data\nTo run CELEBRIMBOR, simply run the below command. Snakemake will read the config.yaml file and run CELEBRIMBOR on the associated files, avoiding Bakta and CheckM as these results have already been generated.\nsnakemake --cores 4 --ignore-incomplete\nNote: we need the --ignore-incomplete flag here as we are not running Bakta or CheckM. For your own analysis outside of this practical, this option is not recommended.\nThis will run for a few minutes. You’ll see a number of tools being run, including MMseqs2 for clustering, and cgt for frequency threshold adjustment.\nWhile you’re waiting, feel free to take a look at the CELEBRIMBOR preprint or PopPUNK paper.\nYou’ll observe the new frequency thresholds printed to the console. Here, the core threshold was reduced from 95% to 92.73%, whilst the rare threshold was increased from 5% to 10%.\nCore threshold: &gt;= 102 observations or &gt;= 92.73% frequency\nRare threshold: &lt;= 11 observations or &lt;= 10.00% frequency\nTake a look at the output files by running :\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && ls\nThe pangenome_summary.tsv describes the assignments of each gene family to a frequency compartment based on the raw frequency values (column order: gene name, gene annotation, frequency, frequency compartment).\ncgt_output.txt details the adjusted frequency compartment assignments calculated by CELEBRIMBOR (column order: gene name, gene count, adjusted frequency compartment).\nUsing R, you can plot the different between the adjusted and unadjusted results by updating the script plot_frequency.R with the paths to pangenome_summary.tsv and cgt_output.txt.\nCopy the R script to the working directory and edit:\ncp ~/course_dir/data_dir/MGnify_training_course/plot_frequency.R ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nvim plot_frequency.R\nEdit the file below :\nlibrary(ggplot2)\n\n# read in data\npangenome.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/pangenome_summary.tsv\", sep = \"\\t\", header=FALSE)\ncolnames(pangenome.summary) &lt;- c(\"gene_name\", \"gene_family\", \"frequency\", \"compartment_freq\")\npangenome.summary$gene_family &lt;- NULL\n\ncgt.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/cgt_output.txt\", sep = \"\\t\", header=TRUE)\ncolnames(cgt.summary) &lt;- c(\"gene_name\", \"count\", \"compartment_adjusted\")\n\n# Merge data\ntotal.df &lt;- merge(pangenome.summary, cgt.summary, by = \"gene_name\")\n\n# stack data\nstacked.df &lt;- data.frame(Type = \"Unadjusted\", Compartment = total.df$compartment_freq)\nstacked.df &lt;- rbind(stacked.df, data.frame(Type = \"Adjusted\", Compartment = total.df$compartment_adjusted))\nstacked.df$Compartment &lt;- factor(stacked.df$Compartment, levels = c(\"rare\", \"middle\", \"core\"))\nstacked.df$Type &lt;- factor(stacked.df$Type, levels = c(\"Unadjusted\", \"Adjusted\"))\n\n# plot data\np &lt;- ggplot(stacked.df, aes(x=Type, fill = Type)) + facet_grid(Compartment~., scales = \"free_y\") + geom_bar() + xlab(\"Pangenome analysis type\") + ylab(\"Gene count\") + theme(legend.position = \"none\")\nggsave(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/adjusted_gene_frequency.png\", plot = p)\nRun the script from the R terminal :\nR\nsource(\"plot_frequency.R\")\nUse Ctrl + Z to close the R terminal.\nTake a look at adjusted_gene_frequency.png. Observe that the core genome and rare genome sizes increase after adjustment, whilst the middle or intermediate decreases.\n\n\n\nEffect of frequency adjustment\n\n\nThere are other files that can be used for downstream analysis, such as presence_absence_matrix.txt which defines in which genomes gene families are found, as well as the annotated directory, which contains gene annotations generated using Bakta.\n\n\nFurther analysis\nTry running CELEBRIMBOR with different parameters, such as cgt_breaks which defines the original rare and core thresholds to adjust, or cgt_error, which defines the false negative rate of CELEBRIMBOR.\nYou can also try running with clustering_method set to panaroo, which uses Panaroo, a more accurate clustering method but less scalable than MMseqs2. Also, when running Panaroo, try setting panaroo_stringency to moderate or senstive. Details on the effect of these parameters can be found here.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/comparative_metagenomics.html",
    "href": "sessions/comparative_metagenomics.html",
    "title": "Comparative metagenomics",
    "section": "",
    "text": "In this practical session, we aim to demonstrate how the MGnifyR tool can be used to fetch data and metadata of a MGnify metagenomic analysis. Then we show diversity metrics calculus and two methods to identify differentially abundant features using taxonomic and functional profiles generated through the MGnify v5.0 pipeline for metagenomic assemblies, as shown in the workflow schema below.\nThe dataset is the TARA ocean metagenomic study (WMS) corresponding to size fractions for prokaryotes MGYS00002008. Find more information about the TARA Ocean Project.\n\n\n\nMgnify assembly analysis pipeline v5.0\n\n\nMGnifyR is a library that provides a set of tools for easily accessing and processing MGnify data in R, making queries to MGnify databases through the MGnify API. The benefits of MGnifyR are that data can either be fetched in TSV format or be directly combined in a phyloseq object to run an analysis in a custom workflow.\nThe exercises are organized into 5 main sections:\n\nFetching data and preprocessing\nNormalization, alpha diversity indices and taxonomic profiles visualization\nComparative metagenomics at community level: Beta diversity\nDetection of differentially abundant taxa (SIAMCAT)\nDetection of differentially abundant functions (Aldex2)",
    "crumbs": [
      "Home",
      "Sessions",
      "Comparative metagenomics"
    ]
  },
  {
    "objectID": "sessions/comparative_metagenomics.html#normalization-methods-alpha-beta-diversity-and-differentially-abundant-features",
    "href": "sessions/comparative_metagenomics.html#normalization-methods-alpha-beta-diversity-and-differentially-abundant-features",
    "title": "Comparative metagenomics",
    "section": "",
    "text": "In this practical session, we aim to demonstrate how the MGnifyR tool can be used to fetch data and metadata of a MGnify metagenomic analysis. Then we show diversity metrics calculus and two methods to identify differentially abundant features using taxonomic and functional profiles generated through the MGnify v5.0 pipeline for metagenomic assemblies, as shown in the workflow schema below.\nThe dataset is the TARA ocean metagenomic study (WMS) corresponding to size fractions for prokaryotes MGYS00002008. Find more information about the TARA Ocean Project.\n\n\n\nMgnify assembly analysis pipeline v5.0\n\n\nMGnifyR is a library that provides a set of tools for easily accessing and processing MGnify data in R, making queries to MGnify databases through the MGnify API. The benefits of MGnifyR are that data can either be fetched in TSV format or be directly combined in a phyloseq object to run an analysis in a custom workflow.\nThe exercises are organized into 5 main sections:\n\nFetching data and preprocessing\nNormalization, alpha diversity indices and taxonomic profiles visualization\nComparative metagenomics at community level: Beta diversity\nDetection of differentially abundant taxa (SIAMCAT)\nDetection of differentially abundant functions (Aldex2)",
    "crumbs": [
      "Home",
      "Sessions",
      "Comparative metagenomics"
    ]
  },
  {
    "objectID": "sessions/comparative_metagenomics.html#open-the-jupyter-notebook",
    "href": "sessions/comparative_metagenomics.html#open-the-jupyter-notebook",
    "title": "Comparative metagenomics",
    "section": "Open the Jupyter Notebook",
    "text": "Open the Jupyter Notebook\nThe practice has been prepared in a Jupyter Notebook available in the MGnify Notebooks Github repo.\n\n\n\n\n\n\nTo access the notebook open a terminal and run the following commands:\n\n\n\ncd ~/mgnify-notebooks\nsudo chown -R training .\ngit stash --all\ngit pull\ngit switch comparative_practice_2023\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nFind and open the ‘comparative_practice_2023’ notebook in the ‘R examples’ directory\n\n\n\n\n\n\n\n\n\n\n\n\nThe notebook has coloured boxes to indicate relevant steps in the analysis or to introduce material for discussion\n\n\n\nYellow boxes:\n\nUp to you: To re-run the analysis changing parameters\n\nQuestions: For open discussion\n\nBlue boxes:\n\nNotes: Informative boxes about the running command\n\nTips: Information useful for results interpretation\n\n\n\n\n\n\n\n\n\n\nTo leave the notebook, you can save the changes and close the window in the browser. Then, terminate the process at the terminal:\n\n\n\nCTRL+C",
    "crumbs": [
      "Home",
      "Sessions",
      "Comparative metagenomics"
    ]
  },
  {
    "objectID": "sessions/comparative_metagenomics.html#use-the-jupyter-notebook-after-the-course",
    "href": "sessions/comparative_metagenomics.html#use-the-jupyter-notebook-after-the-course",
    "title": "Comparative metagenomics",
    "section": "Use the Jupyter Notebook after the course",
    "text": "Use the Jupyter Notebook after the course\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "Comparative metagenomics"
    ]
  }
]