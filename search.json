[
  {
    "objectID": "sessions/pangenome.html",
    "href": "sessions/pangenome.html",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals are available on the virtual machine here:\nls ~/course_dir/data_dir/MGnify_training_course\nActivate the virtual enviroment for this session:\nvirtualenv /home/training/venv/training_env2/bin/activate\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/course_dir/data_dir/MGnify_training_course/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#preamble",
    "href": "sessions/pangenome.html#preamble",
    "title": "Pangenome analysis of metagenomic data",
    "section": "",
    "text": "We will first run through how to use our tool PopPUNK to build models to find subclusters within isolates data, then how to use these models (or pre-existing models) to assign MAG data to these clusters.\nWe anticipate the following timings:\n\nBuilding a PopPUNK model – 45 minutes.\nAssigning using a PopPUNK model – 30 minutes.\n\nPlease do not spend significantly longer than this on each one if you wish to complete the practical. You can move onto the next section at any time.\nWe will then show how to correct MAG data for incompleteness to give each gene a classification of core or accessory.\n\nFinding core and accessory genes from MAG data – 45 minutes.\n\nUse the sidebar to see the instructions for each part.\nThe files for all practicals are available on the virtual machine here:\nls ~/course_dir/data_dir/MGnify_training_course\nActivate the virtual enviroment for this session:\nvirtualenv /home/training/venv/training_env2/bin/activate\nThe course prerequisities are already install on the VM. To activate the environment, run:\nsource ~/course_dir/data_dir/MGnify_training_course/pangenome_mgnify_env/bin/activate\nIf the enviroment has been found correctly, you should see something similar to the following on your terminal\n(pangenome_mgnify_env) training@user:~$",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#building-poppunk-models",
    "href": "sessions/pangenome.html#building-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "1. Building PopPUNK models",
    "text": "1. Building PopPUNK models\nWe will be using 112 B. uniformis isolate genomes (i.e. not MAG data). We are going to use these to define subspecies within the population using PopPUNK. These can be listed using:\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model\nInstallation instructions and an overview are available.\nPopPUNK is already installed on the VM. Confirm this by running poppunk --version.\n\nCreating the database\nThe first step to running PopPUNK on a new species is to create a ‘database’ which contains sketches of the genomes and calculates all of the core and accessory distances between the samples. We will be following the guidance in the relevant section of the documentation.\nFirst, navigate to the working directory and create a new directory:\ncd ~/course_dir/work_dir/Day_5 && mkdir pangenome && cd pangenome\nWe need to create a list of the input files. This needs to have the sample names and the location of files with their genomes. This can be created in many ways, here we will use a simple bash command:\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_build_model/MGYG*) &gt; rfile.txt\nWe can now create the input database with --create-db as follows:\npoppunk --create-db --output b_uniformis_db --r-files rfile.txt --min-k 17 --max-k 33 --plot-fit 10 --threads 4\nThis will run on 4 CPU cores to increase the speed. The other option is the range of k-mer lengths used, see the documentation for more information. Have a look at the plots in the output directory e.g. b_uniformis_db/b_uniformis_db_distanceDistribution.png and some of the fit examples such as b_uniformis_db_fit_example_1.pdf.\nUsually we want to run some quality control on the data, which we can do as follows:\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.05 --max-a-dist 1 --output b_uniformis_db_qc\nThis removes outlying distances and poor quality samples. In this case the data is all good quality and should all be retained.\nHowever if we run with a smaller core (pi) distance, this will remove half of the samples.\npoppunk --qc-db --ref-db b_uniformis_db --max-pi-dist 0.01 --max-a-dist 1 --output b_uniformis_db_qc\nThis isn’t a good idea here as this core distance is too strict for a species as diverse as B. uniformis.\n\n\nFitting a model\nWe now need to create a ‘model’ which determines a cutoff below which genomes are considered the same subspecies.\nThere are many options available, as detailed in the documentation.\nAs we have ‘small’ sample collection with strain-structure where distance distribution components are clearly separated, we’ll try the Bayesian Gaussian Mixture Model with two components:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K2 --K 2\nFrom the output to the console we can see that everything is in one cluster (In Network Summary there is one component) and so we haven’t found any subspecies. Have a look at the output plot b_uniformis_BGMM_K2/b_uniformis_BGMM_K2_DPGMM_fit.png too.\nIt looks like adding an extra component (‘blob’) might help, so let’s try that and make three rather than two:\npoppunk --fit-model bgmm --ref-db b_uniformis_db --output b_uniformis_BGMM_K3 --K 3\nThat looks better and there are some clusters (In Network Summary there are is seven components), but if you look at the same plot again (b_uniformis_BGMM_K3/b_uniformis_BGMM_K3_DPGMM_fit.png) it doesn’t look like a great fit to the data.\nIn this case the data is actually a bit sparse to automatically get a good fit and we can do a much better job if we enrich the dataset with a few thousand MAGs and then use the ‘refine’ model mode. But for now we’ll take a shortcut and impose a cutoff that looks like it will work by using the threshold model:\npoppunk --fit-model threshold --ref-db b_uniformis_db --output b_uniformis_threshold --threshold 0.0025\nIn fact, this way of choosing a model is supported by population genetic simulations (see this paper). So we are acutally free to choose a cutoff that defines subspecies which ‘work’ for us, and the models in PopPUNK are mostly a more convenient way of automating this to find useful clusters in general.\nAs you can see, this is all a bit fiddly and requires iteration. It is usually better to use a previously fitted and tested model, which we will cover in the next part.\n\n\nVisualising the results\nBut before we move on, let’s get a better look at the results. We can make a core genome tree and accessory genome embedding and plot the clusters interactively in the browser. First, run the poppunk_visualise command.\npoppunk_visualise --ref-db b_uniformis_db --model-dir b_uniformis_threshold --microreact --maxIter 100000 --output b_uniformis_db_viz\nHere, maxIter is being used to reduce the number embedding iterations as the dataset is small, just so the command runs quickly.\nNow, open up https://microreact.org/ in your browser and choose ‘Upload’. Drag and drop the .microreact file in the b_uniformis_db_viz directory to see the clusters and the tree. Do they look ok?\n\n\nFurther analysis\nIf you have time, try playing around with different methods of fitting PopPUNK models in the documentation.\nYou can experiment with HDBSCAN, a method for automatically detecting the number and position of clusters.\nOnce you have a model fitted using BGMM or HDBSCAN, you can also refine it. This method takes a previously identified within-strain boundary and moves it to optimise the network score of the strain cluster network. We always recommend refining a previous fit, as it may significantly improve strain assignments.\nYou can also play around with visualisation tools such as Cytoscape. Use the poppunk_visualise tool to generate a cytoscape output following the documentation. This will allow you to visualise the strain cluster network, the components of which are the strains detected by PopPUNK. Note: this will generate a file for each component, as well as the whole network, enabling visualisation of the whole network or just parts of it.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#assigning-using-poppunk-models",
    "href": "sessions/pangenome.html#assigning-using-poppunk-models",
    "title": "Pangenome analysis of metagenomic data",
    "section": "2. Assigning using PopPUNK models",
    "text": "2. Assigning using PopPUNK models\nIt is faster to ‘assign’ new samples to an existing definition of subspecies. This has the bonus that their names will be consistent across studies.\nTypically, you can download an existing database with a fit from https://www.bacpop.org/poppunk/. If you have a fit for a new species please send it to us and we can share it here.\nThere is no fit for B. uniformis (yet…) so we’ll use the one we just made.\n\nUsing poppunk_assign to assign MAGs to subspecies\nNow we’ll work with a large collection of MAGs. These are the B. uniformis MAGs from MGnify with &gt;95% completeness and &lt;5% contamination. They can be listed here\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta/\nPopPUNK distances are relatively robust to missing sequence content seen in MAGs, but less able to deal with contamination.\nAgain, navigate to the work directory and create the list of input files for poppunk:\ncd ~/course_dir/work_dir/Day_5/pangenome\npaste &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta | cut -d \".\" -f 1 ) &lt;(ls ~/course_dir/data_dir/MGnify_training_course/MAGs_to_assign/fasta/*.fasta) &gt; qfile.txt\nOne small problem is that these MAGs also contain the isolates from before. PopPUNK will refuse to assign these without unique names. Here’s a bash incantation to remove the duplicates:\ncut -f 1 rfile.txt &gt; ref_names.txt && grep -v -w -f ref_names.txt qfile.txt &gt; qfile_nodups.txt\nmv qfile_nodups.txt qfile.txt\nThe command is relatively simple, we need to provide the database, the model directory and the input ‘query’ genomes to assign. Quality control is ‘built-in’:\npoppunk_assign --db b_uniformis_db --model-dir b_uniformis_threshold --query qfile.txt --output b_uniformis_queries --threads 4 --max-merge 3 --run-qc --max-a-dist 0.8\nThe b_uniformis_queries_clusters.csv file contains the subspecies assignments.\nThe visualisation command is a bit more involved as we need to point to both directories and the model directory:\npoppunk_visualise --ref-db b_uniformis_db --query-db b_uniformis_queries --model-dir b_uniformis_threshold --microreact --output b_uniformis_query_viz --threads 4 --maxIter 10000000 --previous-query-clustering b_uniformis_queries/b_uniformis_queries_clusters.csv --previous-clustering b_uniformis_threshold/b_uniformis_threshold_clusters.csv\nLoad the .microreact output file in the b_uniformis_query_viz directory in Microreact again to see the output.\nUse the menu under the ‘eye’ to change from reference/query colour (‘Status’) to ‘Cluster_Cluster’ to see the clusters.\n\n\nUpdating the database\nYou’ll see that some MAGs formed new clusters (‘novel clusters’), whilst others were merged, as the new genomes connected to multiple clusters generating in the original database.\nIt is possible to permanently add the query genomes to the database, so future uses make use of novel cluster assignments. Simply add --update-db to the command above. This is beyond the scope of this practical but is documented here and here.\n\n\nFurther analysis\nIf you have time, try generating another visualisation of the query network using Cytoscape. How does this network compare to the original one from the previous section, when you only built using isolate genomes and not MAG data?\nAlso, try building a PopPUNK model as before, but only using the MAG data. How do the distance distributions compare between the isolate data and MAG data? Does a threshold model work with the data, or would an automated method for model fitting work better?\nOnce you have a model fitted using BGMM or HDBSCAN, you can also refine it. This method takes a previously identified within-strain boundary and moves it to optimise the network score of the strain cluster network. We always recommend refining a previous fit, as it may significantly improve strain assignments.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "href": "sessions/pangenome.html#finding-core-and-accessory-genes",
    "title": "Pangenome analysis of metagenomic data",
    "section": "3. Finding core and accessory genes",
    "text": "3. Finding core and accessory genes\nWe can use a probabilistic model to correct for the fact that we know MAGs are missing some genes. If we don’t do this then we will systematically under-estimate their population frequency, and end up with nothing in the core genome.\n\nCELEBRIMBOR: a pipeline for pangenome analysis and threshold correction\nThe CELEBRIMBOR prerequisities are already install on the VM. The source code files can be found in the CELEBRIMBOR directory which can be listed here:\nls /home/training/course_dir/data_dir/MGnify_training_course/CELEBRIMBOR\nAs well be editing the source code, make a copy in your working directory:\ncp -r /home/training/course_dir/data_dir/MGnify_training_course/CELEBRIMBOR ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nThe files we will be analysing can be listed here:\nls ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR\nThis archive contains two directories. CELEBRIMBOR_MAGs contains the fasta files we will be analysing. results contains the Bakta and CheckM outputs which were generated previously from these genomes. As these are the slowest parts of the analysis, we have provided them to allow you to generate results faster.\nAs we’ll be updating results, copy the whole directory to your working directory:\ncp -r ~/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/results ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nCELEBRIMBOR is a snakemake pipeline which enables automated gene annotation, clustering, completeness estimation and core threshold adjustment. Snakemake allows workflows to be re-run if workflows stop prematurely.\nTo prevent CELEBRIMBOR from re-running Bakta and CheckM, I have edited the source code. If you do your own analysis outside of this practical, you will run the full workflow using identical commands.\nDocumentation for CELEBRIMBOR can be found here.\nSnakemake reads a config.yaml file to assign parameters. Navigate to the CELEBRIMBOR directory, and update the config.yaml file with the appropriate parameters.\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR\nvim config.yaml\nTo start typing in vim, type i. To close vim, first use the esc key, and then WITH saving type, :wq, or WITHOUT saving, type :q!\n#output directory\noutput_dir: /home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\n\n# dir with genome fasta files, must have '.fasta' extension, to convert use 'rename .fa .fasta  *.fa' (e.g. if extensions are .fa)\ngenome_fasta: /home/training/course_dir/data_dir/MGnify_training_course/MAGs_for_CELEBRIMBOR/CELEBRIMBOR_MAGs\n\n# path to bakta DB:\nbakta_db: /home/training/course_dir/data_dir/db-light\n\n# cgt executable parameters\ncgt_exe: /home/training/.cargo/bin/cgt_bacpop\ncgt_breaks: 0.05,0.95\ncgt_error: 0.05\n\n# choice of clustering method, either \"mmseqs2\" or \"panaroo\"\nclustering_method: \"mmseqs2\"\n\n# must be one of \"strict\", \"moderate\" or \"sensitive\"\npanaroo_stringency: \"strict\"\nNote: paths in the config.yaml file cannot contain ~ symbols.\nTo run CELEBRIMBOR, simply run the below command. Snakemake will read the config.yaml file and run CELEBRIMBOR on the associated files, avoiding Bakta and CheckM as these results have already been generated.\nsnakemake --cores 4\nThis will run for a few minutes. You’ll see a number of tools being run, including MMseqs2 for clustering, and cgt for frequency threshold adjustment.\nWhile you’re waiting, feel free to take a look at the CELEBRIMBOR preprint or PopPUNK paper.\nYou’ll observe the new frequency thresholds printed to the console. Here, the core threshold was reduced from 95% to 92.73%, whilst the rare threshold was increased from 5% to 10%.\nCore threshold: &gt;= 102 observations or &gt;= 92.73% frequency\nRare threshold: &lt;= 11 observations or &lt;= 10.00% frequency\nTake a look at the output files by running:\ncd ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results && ls\nThe pangenome_summary.tsv describes the assignments of each gene family to a frequency compartment based on the raw frequency values (column order: gene name, gene annotation, frequency, frequency compartment).\ncgt_output.txt details the adjusted frequency compartment assignments calculated by CELEBRIMBOR (column order: gene name, gene count, adjusted frequency compartment).\nUsing R, you can plot the different between the adjusted and unadjusted results by updating the script plot_frequency.R with the paths to pangenome_summary.tsv and cgt_output.txt.\nCopy the R script to the working directory and edit:\ncp ~/course_dir/data_dir/MGnify_training_course/plot_frequency.R ~/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results\nvim plot_frequency.R\nEdit the file below:\nlibrary(ggplot2)\n\n# read in data\npangenome.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/pangenome_summary.tsv\", sep = \"\\t\", header=FALSE)\ncolnames(pangenome.summary) &lt;- c(\"gene_name\", \"gene_family\", \"frequency\", \"compartment_freq\")\npangenome.summary$gene_family &lt;- NULL\n\ncgt.summary &lt;- read.csv(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/cgt_output.txt\", sep = \"\\t\", header=TRUE)\ncolnames(cgt.summary) &lt;- c(\"gene_name\", \"count\", \"compartment_adjusted\")\n\n# Merge data\ntotal.df &lt;- merge(pangenome.summary, cgt.summary, by = \"gene_name\")\n\n# stack data\nstacked.df &lt;- data.frame(Type = \"Unadjusted\", Compartment = total.df$compartment_freq)\nstacked.df &lt;- rbind(stacked.df, data.frame(Type = \"Adjusted\", Compartment = total.df$compartment_adjusted))\nstacked.df$Compartment &lt;- factor(stacked.df$Compartment, levels = c(\"rare\", \"middle\", \"core\"))\nstacked.df$Type &lt;- factor(stacked.df$Type, levels = c(\"Unadjusted\", \"Adjusted\"))\n\n# plot data\np &lt;- ggplot(stacked.df, aes(x=Type, fill = Type)) + facet_grid(Compartment~., scales = \"free_y\") + geom_bar() + xlab(\"Pangenome analysis type\") + ylab(\"Gene count\") + theme(legend.position = \"none\")\nggsave(\"/home/training/course_dir/work_dir/Day_5/pangenome/CELEBRIMBOR_results/adjusted_gene_frequency.png\", plot = p)\nRun the script from the R terminal:\nR\nsource(\"plot_frequency.R\")\nUse Ctrl + Z to close the R terminal.\nTake a look at adjusted_gene_frequency.png by opening the Files application and navigating to Home -&gt; course_dir -&gt; work_dir -&gt; Day_5 -&gt; pangenome -&gt; CELEBRIMBOR_results. Observe that the core genome and rare genome sizes increase after adjustment, whilst the middle or intermediate decreases.\n\n\n\nEffect of frequency adjustment\n\n\nThere are other files that can be used for downstream analysis, such as presence_absence_matrix.txt which defines in which genomes gene families are found, as well as the annotated directory, which contains gene annotations generated using Bakta.\n\n\nFurther analysis\nTry running CELEBRIMBOR with different parameters, such as cgt_breaks which defines the original rare and core thresholds to adjust, or cgt_error, which defines the false negative rate of CELEBRIMBOR.\nYou can also try running with clustering_method set to panaroo, which uses Panaroo, a more accurate clustering method but less scalable than MMseqs2 (this will likely take a while to run). Also, when running Panaroo, try setting panaroo_stringency to moderate or senstive. Details on the effect of these parameters can be found here.",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/pangenome.html#example-analysis",
    "href": "sessions/pangenome.html#example-analysis",
    "title": "Pangenome analysis of metagenomic data",
    "section": "Example analysis",
    "text": "Example analysis\nExample analysis for all of today’s workshop is available here:\nls ~/course_dir/data_dir/MGnify_training_course/example_analysis/pangenome",
    "crumbs": [
      "Home",
      "Sessions",
      "Pangenome analysis of metagenomic data"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html",
    "href": "sessions/genome_uploader.html",
    "title": "The genome_uploader: tutorial",
    "section": "",
    "text": "This session will simulate genomes registration and submission with the genome_uploader, a tool developed within MGnify to facilitate the upload of bins and MAGs to the ENA (European Nucleotide Archive).\nLet’s first move to the genomeuploader root directory. Open a terminal and type:",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#the-genome_uploader-and-its-metadata-handling",
    "href": "sessions/genome_uploader.html#the-genome_uploader-and-its-metadata-handling",
    "title": "The genome_uploader: tutorial",
    "section": "The genome_uploader and its metadata handling",
    "text": "The genome_uploader and its metadata handling\nAs Maira thoroughly explained in the theory session, bin and MAG submission undergoes 3 different steps:\n\nRegister binned and/or MAG samples\nGenerate manifest files\nSubmit assemblies with Webin-CLI\n\nThe genome_uploader takes care of these 3 steps. The first two are executed together, while the third one needs an extra command to submit previously generated files to ENA servers.\n\n\n\n\n\n\nStep\n\n\n\nYour task is to generate a tsv table with sample metadata describing the genomes you want to register and submit.\n\n\nSimilarly to yesterday’s practical, you will be free to insert values as you wish, as long as regular expressions and mandatory fields described in the checklist are respected. You will need to select whether you are uploading bins or MAGs, and select your checklist accordingly.\n\n\n\n\n\n\nNote\n\n\n\nTo register a sample, a relative set of metadata must be filled according to the selected checklist. Some of them are mandatory, while some others are only recommended. The genome_uploader will automatically pick the right checklist depending on the input flag:\n\nGSC MIMAG for MAG samples (-mag)\nENA binned metagenome for binned samples (-bin)\n\n\n\nThe main difference between bins and MAGs lies in the uniqueness and the quality of your data. Within an ENA study, there should only be one MAG per species, which should be the highest quality representative genome per predicted species.\nThe generated table will be similar to the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenome_name\ngenome_path\naccessions\nassembly_software\nbinning_software\nbinning_parameters\nstats_generation_software\ncompleteness\ncontamination\ngenome_coverage\nmetagenome\nco-assembly\nbroad_environment\nlocal_environment\nenvironmental_medium\nrRNA_presence\ntaxonomy_lineage\n\n\n\n\nL_sakei\npath/to/L_sakei.fa.gz\nSRR11910206\nmegahit_v1.2.9\nmetabat_v1.2\ndefault\nCheckM2_v1.0.1\n99.7\n0.38\n27\nmarine sediment metagenome\nFalse\nmarine\ncoast\ncoastal sediment\nTrue\nd__Bacteria;p__Firmicutes;c__Bacilli;o__Lactobacillales;f__Lactobacillaceae;g__Latilactobacillus;s__Lactobacillus sakei\n\n\n\n\nWith columns indicating:\n\ngenome_name: genome id (unique string identifier)\naccessions: run(s) or assembly(ies) the genome was generated from (DRR/ERR/SRRxxxxxx for runs, DRZ/ERZ/SRZxxxxxx for assemblies). If the genome was generated by a co-assembly of multiple runs, separate them with a comma.\nassembly_software: name of the assembly software used, with version (e.g. metaSPAdes_4.0.0)\nbinning_software: name of the binning software used, with version (e.g. metabat_v1.2)\nbinning_parameters: binning parameters\nstats_generation_software: name of the statistics generation software used, with version (e.g.checkM_v1)\ncompleteness: float\ncontamination: float\nrRNA_presence: True/False if all among 5S, 16S, and 23S genes, and at least 18 tRNA genes, have been detected in the genome\nNCBI_lineage: full NCBI lineage, either in tax ids (integers) or strings. Format: x;y;z;…\nmetagenome: needs to be listed in the taxonomy tree here (you might need to press “Tax tree - Show” in the right most section of the page)\nco-assembly: True/False, whether the genome was generated from a co-assembly.\ngenome_coverage : genome coverage against raw reads\ngenome_path: path to genome to upload (already compressed)\nbroad_environment: string (explanation following)\nlocal_environment: string (explanation following)\nenvironmental_medium: string (explanation following)\n\nAccording to ENA checklist’s guidelines, ‘broad_environment’ describes the broad ecological context of a sample - desert, taiga, coral reef, … ‘local_environment’ is more local - lake, harbour, cliff, … ‘environmental_medium’ is either the material displaced by the sample, or the one in which the sample was embedded prior to the sampling event - air, soil, water, … For host-associated metagenomic samples, the three variables can be defined similarly to the following example for the chicken gut metagenome: “chicken digestive system”, “digestive tube”, “caecum”.\nIf your genome was generated from raw reads available on the INSDC (including ENA and GenBank), the genome_uploader will automatically inherit relevant metadata for that sample to make. For example, if you are submitting a MAG generated from read SRR11910206, some of the sample metadata will be inherited for the genome sample registration (e.g. collection_date, isolation_source).\nTake a look at the GCS MIMAG checklist as a reference. You will notice that bins and MAGs checklists are very similar, as mandatory fields are the same. You can compare it with the bins checklist yourself.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#considerations",
    "href": "sessions/genome_uploader.html#considerations",
    "title": "The genome_uploader: tutorial",
    "section": "Considerations",
    "text": "Considerations\nHere we suggest a hypothetical scenario you might want to follow to make the metadata search more interesting\nSuppose your original dataset was small, very small, and it only generated three bins. You’d be surprised but yes, it happens sometimes. Two of these bins represent the same species, but their statistics are extremely different. One assembled quite well, while the other one was highly contaminated. One would be considered “medium quality”, while the other “high quality”.\n\n\n\n\n\n\nTip\n\n\n\nThe INSDC defines a genome as high-quality when:\n\n\n\nAccording to what we previously mentioned, two of these bins could be categorised as MAGs, while the lower-quality bin would stay as a bin.\n\n\n\n\n\n\nTip\n\n\n\nTaxonomies can be listed in either NCBI or GTDB format. An example of valid taxonomies you could use in this scenario could be:\n\nGTDB: d__Bacteria;p__Proteobacteria;c__Gammaproteobacteria;o__Enterobacterales;f__Vibrionaceae;g__Photobacterium;s__Photobacterium piscicola\nNCBI: 1;131567;2759;33154;4751;451864;5204;452284;1538075;162474;742845;55193;76775",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#provided-materials",
    "href": "sessions/genome_uploader.html#provided-materials",
    "title": "The genome_uploader: tutorial",
    "section": "Provided materials",
    "text": "Provided materials\nYou can follow this link to find a Google Spreasheet called genome_uploader_template.\nThis spreadsheet contains a tab for every webin account assigned to this course’s users. You should use the same accounts that have been assigned yesterday - you will find the list here or in Maira’s documentation. Identify the tab assigned to your webin profile. You can use the table template in that tab to prepare a tsv as input file for the genome_uploader. Once you have filled in the table with all required values, export it as tsv (File –&gt; Download –&gt; Tab-separated values (.tsv)).\n\n\n\n\n\n\nWarning\n\n\n\nTwo tables need to be generated if you are submitting bins and MAGs - one for bins and one for MAGs.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-your-genomes",
    "href": "sessions/genome_uploader.html#registering-your-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Registering your genomes",
    "text": "Registering your genomes\n\n\n\n\n\n\nWarning\n\n\n\nAs you did yesterday, you will need to generate a study from your webin profile, and add it to the following command under UPLOAD_STUDY. You can either look it up on Maira’s documentation from yesterday, or screenshotted at the end of this page.\n\n\nAfter generating the table, it will be time to launch the genome_uploader. You can do so by opening a terminal and typing:\npython genome_upload.py -u UPLOAD_STUDY \\\n--genome_info METADATA_FILE (--mags | --bins) \\\n--webin WEBIN_ID --password PASSWORD \\\n--centre_name CENTRE_NAME [--out] [--force]\nWhere:\n\n-u UPLOAD_STUDY: study accession for genomes upload to ENA (in format ERPxxxxxx or PRJEBxxxxxx)\n---genome_info METADATA_FILE : genomes metadata file in tsv format\n-m, --mags, --b, --bins: select EITHER OF THESE for either bin or MAG upload\n--out: output folder (default: working directory)\n--force: sample xmls won’t be regenerated automatically if a previous xml already exists. If any metadata or value in the tsv table changes, --force will allow xml regeneration.\n--webin WEBIN_ID: webin id (format: Webin_XXXXX)\n--password PASSWORD: webin password\n--centre_name CENTRE_NAME: name of the centre generating and uploading genomes\n\nThis step will also generate manifest files.",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#submitting-your-genomes",
    "href": "sessions/genome_uploader.html#submitting-your-genomes",
    "title": "The genome_uploader: tutorial",
    "section": "Submitting your genomes",
    "text": "Submitting your genomes\nAfter checking that all needed manifests exist, it is necessary to use ENA’s webin-cli resource to upload genomes.\njava -jar ~/JAR/webin-cli-8.0.0.jar \\\n-context=genome -manifest=MANIFEST_FILE \\\n-userName=\"Webin-XXX\" -password=\"YYY\" \\\n-test -submit",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/genome_uploader.html#registering-a-study",
    "href": "sessions/genome_uploader.html#registering-a-study",
    "title": "The genome_uploader: tutorial",
    "section": "Registering a study",
    "text": "Registering a study",
    "crumbs": [
      "Home",
      "Sessions",
      "The genome_uploader: tutorial"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html",
    "href": "sessions/mag_catalogues.html",
    "title": "MAG Catalogues",
    "section": "",
    "text": "MAGs 1 are an approach to deriving genome-resolved information from metagenomic datasets.\nMGnify’s MAG Catalogues are biome-specific, clustered, annotated collections of MAGs. Biomes are selected on the grounds of data availability, community interest, and project objectives.\n\n\n\n\n\n\n\n\nSearch the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.\n\n\n\n\n\nSourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5\n\n\n\n\n\nThe MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "href": "sessions/mag_catalogues.html#practical-1-finding-mags-by-taxonomy-on-the-mgnify-website",
    "title": "MAG Catalogues",
    "section": "",
    "text": "Search the MGnify website\n\n\n\nSearch the All genomes list for the genus Jonquetella\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is that genus found?\nWhat do thise biomes have in common, and how does this align with the species found? 2\n\n\nNow, we want to get a FASTA sequence for this genus.\n\n\n\n\n\n\nFind the “best” MAG\n\n\n\nUsing what we’ve learned about QC on the course, look at the detail statistics of the Jonquetella MAGs. Which one is best? 3\n\n\n\n\n\n\n\n\nDownload the DNA sequence FASTA file of the “best” MAG\n\n\n\nWe will use it later.",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "href": "sessions/mag_catalogues.html#practical-2-query-mgnify-catalogues-using-sourmash",
    "title": "MAG Catalogues",
    "section": "",
    "text": "Sourmash is a tool to compare DNA sequences against each other. The MGnify Genomes resource uses the sourmash library to create sketches (hashes) of every genome in every catalogues. You can query this index using your own sequences (typically MAGs you have retrieved from elsewhere or assembled yourself).\n\n\n\n\n\n\nQuery the catalogues using the Jonquetella MAG\n\n\n\nUse the MAG sequence FASTA file you earlier retrieved. 4\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn which catalogues is a match found for that query genome?\nWhat use cases can you think of for this kind of cross-catalogue search? 5",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "href": "sessions/mag_catalogues.html#practical-3-query-mgnify-catalogues-using-sourmash-programmatically",
    "title": "MAG Catalogues",
    "section": "",
    "text": "The MGnify website is just a client of the MGnify API 6.\nFor this part of the practical, there is a Jupyter Notebook you can follow along and try to complete the code blocks.\nTo open it on your training VM:\n\n\n\n\n\n\nStep\n\n\n\ncd ~/mgnify-notebooks\ngit status\n# make sure you're on the \"comparative_practice_2023\" branch\ntask edit-notebooks\nAfter a few seconds, some URLs will be printed in the terminal. Open the last one (http://127.0.0.1:8888/lab?token=.....), by right-clicking on the URL and selecting “Open Link”, or by copying-and-pasting it into a web browser like Chromium/Firefox.\n\n\n\n\n\n\n\n\nFind and open the ‘Search MGnify Genomes (course practical 2023)’ notebook in the ‘Python examples’ directory.\n\n\n\nFollow along the steps (completing some code blocks) in the notebook.\n\n\n\n\nThis notebook is based on a publicly accessible version. You can use this at any time.\n\nIt is available to use from your web browser, no installation needed: notebooks.mgnify.org\nYou can see a completed version of it, with all the outputs, on docs.mgnify.org\nYou can use a prebuilt docker image and our public notebooks repository: github.com/ebi-metagenomics/notebooks. This should work on any computer you can install Docker on.\nYou can try and install all the dependencies yourself ¯\\_(ツ)_/¯",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mag_catalogues.html#footnotes",
    "href": "sessions/mag_catalogues.html#footnotes",
    "title": "MAG Catalogues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMetagenome Assembled Genomes↩︎\nHint… what does anthropi in the species J. anthropi derive from?↩︎\nHint… each MAG’s detail page overview tab shows stats including completeness, contamination, and N50.↩︎\nIf you got lost earlier, download it from MGYG000304175.fna↩︎\nThere are interesting use cases for researchers (checking which environments a species is found in, checking whether a newly assembled genome is novel etc), as well as use cases for services like MGnify (cross-linking genomes between catalogues where those datasets are not clustered together).↩︎\nApplication Programming Interface↩︎",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Catalogues"
    ]
  },
  {
    "objectID": "sessions/mags.html",
    "href": "sessions/mags.html",
    "title": "MAG Generation",
    "section": "",
    "text": "Generation of metagenome assembled genomes (MAGs) from assemblies\nAssessment of quality\nTaxonomic assignment\n\n\n\nFor this tutorial, you will need to start the docker container by running the following command in the terminal:\nsudo docker run --rm -it -v /home/training/Binning:/opt/data quay.io/microbiome-informatics/mags-practical-2024:v2\npassword: training\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with СheckM and CheckM2, and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to generate the input.fastq.sam.bam file yourself, you would run the following commands:\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step. In the interest of time, we\n# suggest that if you would like to try the commands, you run them after you complete the practical.\n\n# If you would like to practice generating the bam file, back up the input.fastq.sam.bam file that we\n# provided first, as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\ngunzip ERR011322_1.fastq.gz\ngunzip ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq ERR011322_2.fastq &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of bin.[0-9].fa.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?\n\n\nObviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data_2015_01_16.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nThis command uses a lot of memory. Do not run anything else while executing it.\ncd /opt/data/assemblies\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**. We usually use 50% as the lowest acceptable cut-off (QS50)\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nExample output of CheckM\n\n\n\n\n\nToday researchers also use CheckM2, an improved method of predicting genome quality that uses machine learning. The execution of CheckM2 takes more time than CheckM. We have pre-generated the tab-delimited quality table. It is available on our FTP.\n\n\n\n\n\n\nStep\n\n\n\nDownload the CheckM2 result table that we have pre-generated:\n# create a folder for the CheckM2 result\ncd /opt/data/assemblies\nmkdir checkm2\ncd checkm2\n# download CheckM2 TSV result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2024/mags/checkm2_quality_report.tsv\n\n\nIf you would like to try CheckM2, launch it in a new terminal window:\ncd /opt/data/assemblies\ndocker run -t -v /home/training/Binning:/opt/data quay.io/biocontainers/checkm2:1.0.2--pyh7cba7a3_0 checkm2 predict -i /opt/data/assemblies/contigs.fasta.metabat-bins2000 -o checkm2_result --database_path /opt/data/uniref100.KO.1.dmnd -x fa --force --lowmem -t 4\nWe can compare CheckM and CheckM2 results using a scatter plot. We will plot completeness on the x-axis and contamination on the y-axis. We have created a simple python script to plot CheckM and CheckM2 results separately. Input table should be in the CSV format and it should contain 3 columns: bin name, completeness, and contamination. Required file header: “bin,completeness,contamination”\n\n\n\n\n\n\nStep\n\n\n\nModify the CheckM results table and generate a plot:\n# modify MAGs_checkm.tab\n# add header\necho \"bin,completeness,contamination\" &gt; checkm1_quality_report.csv\n# take the file without the header; leave only columns 1, 12 and 13; replace tabs with commas\ntail -n+2 MAGs_checkm.tab | cut -f1,12,13 | tr '\\t' ',' &gt;&gt; checkm1_quality_report.csv\n\n# plot (apologies for the typo in the script name...)\ncomplenetess_vs_contamination.py -i checkm1_quality_report.csv -o checkm1\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow do the same for CheckM2. Modify the table and make a plot:\n# create CSV\n# add a header\necho \"bin,completeness,contamination\" &gt; checkm2_quality_report.csv\n# take the file without the header, leave only the first 3 columns, replace tabs with commas\ntail -n+2 checkm2_quality_report.tsv | cut -f1-3  | tr '\\t' ',' &gt;&gt; checkm2_quality_report.csv\n\n# plot\ncomplenetess_vs_contamination.py -i checkm2_quality_report.csv -o checkm2\n\n\nYou should now have files checkm1.png and checkm2.png to compare the quality predictions between the two tools. To open the png files, use the file browser (grey folder icon in the left-hand side menu) and navigate to /Home/Binning/assemblies\n\n\n\n\n\n\nQuestion\n\n\n\nDid CheckM and CheckM2 produce similar results? What can you say about the quality of the bins?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#prerequisites",
    "href": "sessions/mags.html#prerequisites",
    "title": "MAG Generation",
    "section": "",
    "text": "For this tutorial, you will need to start the docker container by running the following command in the terminal:\nsudo docker run --rm -it -v /home/training/Binning:/opt/data quay.io/microbiome-informatics/mags-practical-2024:v2\npassword: training",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "href": "sessions/mags.html#generating-metagenome-assembled-genomes",
    "title": "MAG Generation",
    "section": "",
    "text": "Note\n\n\n\nLearning Objectives - in the following exercises, you will learn how to bin an assembly, assess the quality of this assembly with СheckM and CheckM2, and then visualize a placement of these genomes within a reference tree.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs with the assembly process, there are many software tools available for binning metagenome assemblies. Examples include, but are not limited to:\n\nMaxBin\nCONCOCT\nCOCACOLA\nMetaBAT\n\nThere is no clear winner between these tools, so it is best to experiment and compare a few different ones to determine which works best for your dataset. For this exercise, we will be using MetaBAT (specifically, MetaBAT2). The way in which MetaBAT bins contigs together is summarized in Figure 1.\n\n\n\n\n\nFigure 1. MetaBAT workflow (Kang, et al. PeerJ 2015).\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\nPrior to running MetaBAT, we need to generate coverage statistics by mapping reads to the contigs. To do this, we can use bwa and then the samtools software to reformat the output. This can take some time, so we have run it in advance.\n\n\n\n\n\n\n\n\nStep\n\n\n\nLet’s browse the files that we have prepared:\ncd /opt/data/assemblies/\nls\nYou should find the following files in this directory:\n\ncontigs.fasta: a file containing the primary metagenome assembly produced by metaSPAdes (contigs that haven’t been binned)\ninput.fastq.sam.bam: a pre-generated file that contains reads mapped back to contigs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to generate the input.fastq.sam.bam file yourself, you would run the following commands:\n\n# NOTE: you will not be able to run subsequent steps until this workflow is completed because you need\n# the input.fastq.sam.bam file to calculate contig depth in the next step. In the interest of time, we\n# suggest that if you would like to try the commands, you run them after you complete the practical.\n\n# If you would like to practice generating the bam file, back up the input.fastq.sam.bam file that we\n# provided first, as these steps will take a while:\n\ncd /opt/data/assemblies/\nmv input.fastq.sam.bam input.fastq.sam.bam.bak\n\n# index the contigs file that was produced by metaSPAdes:\nbwa index contigs.fasta\n\n# fetch the reads from ENA\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR011/ERR011322/ERR011322_2.fastq.gz\n\ngunzip ERR011322_1.fastq.gz\ngunzip ERR011322_2.fastq.gz\n\n# map the original reads to the contigs:\nbwa mem contigs.fasta ERR011322_1.fastq ERR011322_2.fastq &gt; input.fastq.sam\n\n# reformat the file with samtools:\nsamtools view -Sbu input.fastq.sam &gt; junk \nsamtools sort junk -o input.fastq.sam.bam\n\n\n\n\n\n\n\n\n\n\n\nCreate a subdirectory where files will be saved to\n\n\n\ncd /opt/data/assemblies/\nmkdir contigs.fasta.metabat-bins2000\n\n\nIn this case, the directory might already be part of your VM, so do not worry if you get an error saying the directory already exists. You can move on to the next step.\n\n\n\n\n\n\nStep\n\n\n\nRun the following command to produce a contigs.fasta.depth.txt file, summarizing the output depth for use with MetaBAT:\njgi_summarize_bam_contig_depths --outputDepth contigs.fasta.depth.txt input.fastq.sam.bam\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow let’s put together the metaBAT2 command. To see the available options, run:\nmetabat2 -h\nHere is what we are trying to do:\n\nwe want to bin the assembly file called contigs.fasta\nthe resulting bins should be saved into the contigs.fasta.metabat-bins2000 folder\nwe want the bin file names to start with the prefix bin\nwe want to use the contig depth file we just generated (contigs.fasta.depth.txt)\nthe minimum contig length should be 2000\n\nTake a moment to put your command together but please check the answer below before running it to make sure everything is correct.\n\n\nSee the answer\n\nmetabat2 --inFile contigs.fasta --outFile contigs.fasta.metabat-bins2000/bin --abdFile contigs.fasta.depth.txt --minContig 2000\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce the binning process is complete, each bin will be grouped into a multi-fasta file with a name structure of bin.[0-9].fa.\n\n\n\n\n\n\n\n\nStep\n\n\n\nInspect the output of the binning process.\nls contigs.fasta.metabat-bins2000/bin*\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did the process produce?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each bin?\n\n\nObviously, not all bins will have the same level of accuracy since some might represent a very small fraction of a potential species present in your dataset. To further assess the quality of the bins, we will use CheckM.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheckM has its own reference database of single-copy marker genes. Essentially, based on the proportion of these markers detected in the bin, the number of copies of each, and how different they are, it will determine the level of completeness, contamination, and strain heterogeneity of the predicted genome.\n\n\n\n\n\n\n\n\nStep\n\n\n\nBefore we start, we need to configure CheckM.\ncd /opt/data\nmkdir /opt/data/checkm_data\ntar -xf checkm_data_2015_01_16.tar.gz -C /opt/data/checkm_data\ncheckm data setRoot /opt/data/checkm_data\n\n\nThis program has some handy tools not only for quality control but also for taxonomic classification, assessing coverage, building a phylogenetic tree, etc. The most relevant ones for this exercise are wrapped into the lineage_wf workflow.\n\n\n\n\n\n\nStep\n\n\n\nThis command uses a lot of memory. Do not run anything else while executing it.\ncd /opt/data/assemblies\ncheckm lineage_wf -x fa contigs.fasta.metabat-bins2000 checkm_output --tab_table -f MAGs_checkm.tab --reduced_tree -t 4\nDue to memory constraints (&lt; 40 GB), we have added the option --reduced_tree to build the phylogeny with a reduced number of reference genomes.\nOnce the lineage_wf analysis is done, the reference tree can be found in checkm_output/storage/tree/concatenated.tre.\nAdditionally, you will have the taxonomic assignment and quality assessment of each bin in the file MAGs_checkm.tab with the corresponding level of completeness, contamination, and strain heterogeneity (Fig. 2). A quick way to infer the overall quality of the bin is to calculate the level of **(completeness - 5*contamination). You should be aiming for an overall score of at least 70-80%**. We usually use 50% as the lowest acceptable cut-off (QS50)\nYou can inspect the CheckM output with:\ncat MAGs_checkm.tab\n\n\n\n\n\nExample output of CheckM\n\n\n\n\n\nToday researchers also use CheckM2, an improved method of predicting genome quality that uses machine learning. The execution of CheckM2 takes more time than CheckM. We have pre-generated the tab-delimited quality table. It is available on our FTP.\n\n\n\n\n\n\nStep\n\n\n\nDownload the CheckM2 result table that we have pre-generated:\n# create a folder for the CheckM2 result\ncd /opt/data/assemblies\nmkdir checkm2\ncd checkm2\n# download CheckM2 TSV result\nwget https://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/metagenomics_2024/mags/checkm2_quality_report.tsv\n\n\nIf you would like to try CheckM2, launch it in a new terminal window:\ncd /opt/data/assemblies\ndocker run -t -v /home/training/Binning:/opt/data quay.io/biocontainers/checkm2:1.0.2--pyh7cba7a3_0 checkm2 predict -i /opt/data/assemblies/contigs.fasta.metabat-bins2000 -o checkm2_result --database_path /opt/data/uniref100.KO.1.dmnd -x fa --force --lowmem -t 4\nWe can compare CheckM and CheckM2 results using a scatter plot. We will plot completeness on the x-axis and contamination on the y-axis. We have created a simple python script to plot CheckM and CheckM2 results separately. Input table should be in the CSV format and it should contain 3 columns: bin name, completeness, and contamination. Required file header: “bin,completeness,contamination”\n\n\n\n\n\n\nStep\n\n\n\nModify the CheckM results table and generate a plot:\n# modify MAGs_checkm.tab\n# add header\necho \"bin,completeness,contamination\" &gt; checkm1_quality_report.csv\n# take the file without the header; leave only columns 1, 12 and 13; replace tabs with commas\ntail -n+2 MAGs_checkm.tab | cut -f1,12,13 | tr '\\t' ',' &gt;&gt; checkm1_quality_report.csv\n\n# plot (apologies for the typo in the script name...)\ncomplenetess_vs_contamination.py -i checkm1_quality_report.csv -o checkm1\n\n\n\n\n\n\n\n\nStep\n\n\n\nNow do the same for CheckM2. Modify the table and make a plot:\n# create CSV\n# add a header\necho \"bin,completeness,contamination\" &gt; checkm2_quality_report.csv\n# take the file without the header, leave only the first 3 columns, replace tabs with commas\ntail -n+2 checkm2_quality_report.tsv | cut -f1-3  | tr '\\t' ',' &gt;&gt; checkm2_quality_report.csv\n\n# plot\ncomplenetess_vs_contamination.py -i checkm2_quality_report.csv -o checkm2\n\n\nYou should now have files checkm1.png and checkm2.png to compare the quality predictions between the two tools. To open the png files, use the file browser (grey folder icon in the left-hand side menu) and navigate to /Home/Binning/assemblies\n\n\n\n\n\n\nQuestion\n\n\n\nDid CheckM and CheckM2 produce similar results? What can you say about the quality of the bins?",
    "crumbs": [
      "Home",
      "Sessions",
      "MAG Generation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metagenomics bioinformatics at MGnify (2024)",
    "section": "",
    "text": "This is the course material for the Metagenomics bioinformatics at MGnify course (2024).\n\nOverview\nGain knowledge of the tools, processes and analysis approaches used in the field of metagenomics.\nThis course will cover the metagenomics data analysis workflow from the point of newly generated sequence data. Participants will explore the use of publicly available resources and tools to manage, share, analyse and interpret metagenomics data. The content will include issues of data quality control and how to submit to public repositories. While sessions will detail marker-gene and whole-genome shotgun (WGS) approaches; the primary focus will be on assembly-based approaches. Discussions will also explore considerations when assembling genome data, the analysis that can be carried out by MGnify on such datasets, and what downstream analysis options and tools are available"
  },
  {
    "objectID": "sessions/qc.html",
    "href": "sessions/qc.html",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nActivate virtual environment\n\n\n\nFirstly, we need to create a virtual environment for this session so everything we do for this practical is contained within this environment.\nsource /home/training/venv/training_env5/bin/activate\n\n#check /home/training is in your environment\nls /home/training\n\n\n\n\n\n\n\n\nCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/QC_session/quality. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/QC_session/quality. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/QC_session\ncd /home/training/QC_session\n\nwget http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz\n\ntar -xzvf quality.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the quality.tar.gz\nrm quality.tar.gz\n\n\n\n\n\n\n\n\n\nKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/QC_session/qc_results\nYou should now have in your /home/training/QC_session directory, the following subdirectories (quality and qc_results). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/quality) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/QC_session/quality\nchmod -R 777 /home/training/QC_session/quality\nexport DATADIR=/home/training/QC_session/quality\nchmod -R 777 /home/training/QC_session/qc_results\nexport RESDIR=/home/training/QC_session/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important you that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/qc-practical-2024:v2",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#prerequisites",
    "href": "sessions/qc.html#prerequisites",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "",
    "text": "These instructions are for the course VM. To run externally, please refer to the section at the end.\n\n\n\n\n\n\nA note about copying and pasting in the VMs\n\n\n\nYou will not be able to use Ctrl+C for copying and Ctrl+P for pasting in the VM terminals. Instead, we recommend using right click and selecting copy/paste.\n\n\n\n\n\n\n\n\nActivate virtual environment\n\n\n\nFirstly, we need to create a virtual environment for this session so everything we do for this practical is contained within this environment.\nsource /home/training/venv/training_env5/bin/activate\n\n#check /home/training is in your environment\nls /home/training\n\n\n\n\n\n\n\n\nCheck data is loaded correctly\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/QC_session/quality. Check if it is there. If not, go to the Downloading data section\n\n\n\n\n\n\n\n\nDownloading data\n\n\n\n\n\nFor this practical we will need to a sample dataset. The sample dataset should already be in /home/training/QC_session/quality. If not, you should download and decompress the data as follows:\nmkdir -p /home/training/QC_session\ncd /home/training/QC_session\n\nwget http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz\n\ntar -xzvf quality.tar.gz\n# you should now have the quality subdirectory in your /home/training/QC_session. All the data you will need to run the practicals will be in this subdirectory\n\n#you can now remove the quality.tar.gz\nrm quality.tar.gz\n\n\n\n\n\n\n\n\n\nKeeping your results organised\n\n\n\nThis practical (and the others) will generate quite a few output files from the different commands. It’s therefore recommended you keep your results well organised into different subdirectories, starting with the creation of a qc_results directory that will contain everything else.\nmkdir /home/training/QC_session/qc_results\nYou should now have in your /home/training/QC_session directory, the following subdirectories (quality and qc_results). Make sure both are there before moving onto the next steps. You might run into permission issues later if you have not created them properly.\n\n\nFor this tutorial, you’ll need to move into the working directory (/home/training/quality) and start a Docker container. Set the variables DATADIR and RESDIR as instructed.\ncd /home/training/QC_session/quality\nchmod -R 777 /home/training/QC_session/quality\nexport DATADIR=/home/training/QC_session/quality\nchmod -R 777 /home/training/QC_session/qc_results\nexport RESDIR=/home/training/QC_session/qc_results\nxhost +\nYou will get the message “access control disabled, clients can connect from any host”\n\n\n\n\n\n\nImportant\n\n\n\nAgain, to avoid permission issues, it’s very important you that the directories $DATADIR and $RESDIR variables are exported correctly before running the container. You can check this by running:\necho $DATADIR\necho $RESDIR\nThese commands should print out the paths for those variables. If it’s not printing anything, go back to the last instruction before proceeding.\n\n\nNow start the Docker container:\ndocker run --rm -it --user 1001 -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/qc-practical-2024:v2",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "href": "sessions/qc.html#quality-control-and-filtering-of-the-raw-sequencing-read-files",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Quality control and filtering of the raw sequencing read files",
    "text": "Quality control and filtering of the raw sequencing read files\n\n\n\n\n\n\nLearning Objectives\n\n\n\nIn the following exercises, you’ll learn how to check the quality of short read sequences, identify adaptor sequences, remove adapters and low-quality sequences, and construct a reference database for host decontamination.\n\n\n\n\n\n\n\n\nHere you should see the contents of the working directory.\n\n\n\nThese are the files we’ll use for the practical:\nls /opt/data\n\n\n\nQuality assessment with FastQC and multiqc\nWe will start by using a tool called FastQC, which will generate a report describing multiple quality measures for the given reads.\n\n\n\n\n\n\nGenerate a directory of the FastQC results\n\n\n\nmkdir -p /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_1_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/data/oral_human_example_2_splitaa.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\nThe -o option used with FastQC sends the output files to the given path.\n\n\n\n\n\n\nNow on your computer, select the folder icon.\n\n\n\nNavigate to Files → Home → QC_session → qc_results → fastqc_results → oral in your VM\nRight-click on file oral_human_example_1_splitaa_fastqc.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of FastQC\n\n\nSpend some time looking at the ‘Per base sequence quality.’\n\n\nFor each position, a BoxWhisker-type plot is drawn:\n\nThe central red line is the median value.\nThe yellow box represents the inter-quartile range (25-75%).\nThe upper and lower whiskers represent the 10% and 90% points.\nThe blue line represents the mean quality.\n\nThe y-axis on the graph shows the quality scores. The higher the score, the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it’s common to see base calls falling into the orange area towards the end of a read.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this tell you about your sequence data? When do the errors start?\n\n\nIn the pre-processed files, we see two warnings, as shown on the left side of the report. Navigate to the “Per bases sequence content.”\n\n\n\nScreenshot of FastQC\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAt around 15-19 nucleotides, the DNA composition becomes very even; however, at the 5’ end of the sequence, there are distinct differences. Why do you think that is?\n\n\n\n\n\n\n\n\nStep\n\n\n\nOpen up the FastQC report corresponding to the reversed reads.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any significant differences between the forward and reverse files?\n\n\nFor more information on the FastQC report, please consult the ‘Documentation’ available from this site: FastQC Documentation\nWe are currently only looking at two files, but often we want to look at many files. The tool multiqc aggregates the FastQC results across many samples and creates a single report for easy comparison. Here we will demonstrate the use of this tool.\n\n\n\n\n\n\nRun\n\n\n\nmkdir -p /opt/results/multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/multiqc_results/oral\nchown 1001 /opt/results/multiqc_results/oral/*.html\n\n\nIn this case, we provide the folder containing the FastQC results to multiqc, and similar to FastQC, the -o argument allows us to set the output directory for this summarized report.\n\n\n\n\n\n\nNow on your computer, select the folder icon.\n\n\n\nNavigate to Home → QC_session → qc_results → multiqc_results → oral in your VM\nRight-click on file multiqc_report.html, select ‘open with other application’, and open with Firefox.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as two separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat fraction of reads are duplicates?\n\n\n\n\nQuality filtering with fastp\nSo far we have looked at the raw files and assessed their content, but we have not done anything about removing duplicates, sequences with low quality scores, or removal of the adaptors. So, let’s start this process.\nOur first step will be to perform quality filtering of the reads using a tool called fastp, which is versatile, easy to use, and fast.\n\n\n\n\n\n\nCreate directories that will store output files from the cleaning process\n\n\n\nmkdir -p /opt/results/cleaned/oral\n\n\nThe fastp command you will run contains multiple parameters, so let’s slowly deconstruct it:\n\n\n\n\n\n\nRun\n\n\n\nfastp --in1 /opt/data/oral_human_example_1_splitaa.fastq.gz \\\n      --in2 /opt/data/oral_human_example_2_splitaa.fastq.gz \\\n      --out1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/oral/oral.fastp.json --html /opt/results/cleaned/oral/oral.fastp.html\n\n\n\n--in1/--in2 — The two input paired-end read files\n--out1/--out2 — The two output files after filtering\n-l — Minimum read length required, reads below 50 in this case are discarded\n--cut_right/--cut_right_window_size/--cut_right_mean_quality — These three options all work together. --cut_right creates a window, of size specified by --cut_right_window_size, which will slide from the front to the tail of the reads, calculating the mean quality score in the window at every step. If at any point, the mean quality score value is lower than the one specified by --cut_right_mean_quality, then the bases of that window and everyting to its right are immediately discarded for that read.\n-t 1 — Will trim the tail of its final base, as it’s a lot lower quality than other positions. This is a setting you should set very purposefully and for good reason, like we’re doing here.\n--detect_adapter_for_pe — One of the very useful features of fastp is that it can detect adapters automatically and remove them, which this parameter activates.\n--json/--html — Outputs a summary report similar to FastQC, in both .json and .html formats.\n\n\n\n\n\n\n\nQuestion\n\n\n\nFind and open the .html report. How many reads were removed? How has the average quality of the reads changed? \n\n\n\n\nDecontamination with bowtie2\nNext, we want to remove any potential contamination in our reads, which we’ll do using a tool called bowtie2. It is always good to routinely screen for human DNA (which may come from the host and/or staff performing the experiment). However, if the sample is from a mouse, you would want to download the mouse genome. The first step in the decontamination process is therefore to make a database that our reads will be searched against for sources of contamination.\nIn the following exercise, we are going to use two “genomes” already downloaded for you in the decontamination folder. To make this tutorial quicker and smaller in terms of file sizes, we are going to use PhiX (a common spike-in) and just chromosome 10 from human.\n\n\n\n\n\n\nThe reference sequences files we’ll be using\n\n\n\nls /opt/data/decontamination\n\n# Output: GRCh38_chr10.fasta  phix.fasta\n\n\nFor the next step, we need one file, so we want to merge the two different fasta files. This is simply done using the command-line tool cat.\n\n\n\n\n\n\nRun\n\n\n\ncat /opt/data/decontamination/GRCh38_chr10.fasta /opt/data/decontamination/phix.fasta &gt; /opt/data/decontamination/chr10_phix.fasta\n\n\nYou will often need to build indices for large sequence files - including sequencing files and reference files - to speed up computation. To build a bowtie index for our new concatenated PhiX-chr10 file, run the following script. NOTE. The indexing step can take a while to run (~ 2 -3 minutes for the example used in this practical)\n\n\n\n\n\n\nRun\n\n\n\nbowtie2-build /opt/data/decontamination/chr10_phix.fasta /opt/data/decontamination/chr10_phix.index\n\n#check output - indexed files end with *bt2\nls /opt/data/decontamination/chr10_phix.index*bt2\n\n# opt/data/decontamination/chr10_phix.index.1.bt2\n# opt/data/decontamination/chr10_phix.index.2.bt2\n# opt/data/decontamination/chr10_phix.index.3.bt2\n# opt/data/decontamination/chr10_phix.index.4.bt2\n# opt/data/decontamination/chr10_phix.index.rev.1.bt2\n# opt/data/decontamination/chr10_phix.index.rev.2.bt2\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible to automatically download a pre-indexed human genome in bowtie2. Commonly-used bowtie2 indices can be downloaded from https://bowtie-bio.sourceforge.net/bowtie2/index.shtml.\n\n\nNow we are going to use our new indexed chr10_phix reference and decontaminate our already quality-filtered reads from fastp. Run bowtie2 as below. NOTE. This alignment step can take a few minutes to run.\n\n\n\n\n\n\nRun\n\n\n\nbowtie2 -1 /opt/results/cleaned/oral/oral_human_example_1_splitaa.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/oral/oral_human_example_2_splitaa.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/oral/oral_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n-1 - input read 1\n-2 - input read 2\n-x - reference genome index filename prefix (minus trailing .X.bt2)\n--un-con-gz - write pairs that didn’t align concordantly to assigned  #this will be your cleaned reads\n--very-sensitive - set stringent parameters to call reads as a mapped read (Same as -D 20 -R 3 -N 0 -L 20 -i S,1,0.50)\n--dovetail - concordant when mates extend past each other (ie. the paired alignments overlaps one another )\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFrom the bowtie2 output on the terminal, what fraction of reads have been deemed to be contaminating?\n\n\nbowtie2 changes the naming scheme of the output files, so we rename them to be consistent:\n\n\n\n\n\n\nRun\n\n\n\nmv /opt/results/cleaned/oral/oral_human_example.fastq.1.gz /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/oral/oral_human_example.fastq.2.gz /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz\n\n\n\n\nPost-QC assessment with FastQC and multiqc\n\n\n\n\n\n\nRun FastQC\n\n\n\nUsing what you have learned previously, generate a FastQC report for each of the *trimmed_decontam.fastq.gz files. Output the new fastqc report files in the same /opt/results/fastqc_results/oral directory as last time.\n\n\n\n\n\n\n\n\nRun FastQC (code)\n\n\n\n\n\nfastqc /opt/results/cleaned/oral/oral_human_example_1_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nfastqc /opt/results/cleaned/oral/oral_human_example_2_splitaa_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/oral\nchown 1001 /opt/results/fastqc_results/oral/*.html\n\n\n\n\n\n\n\n\n\nRun multiQC\n\n\n\nAlso generate a multiQC report, with /opt/results/fastqc_results/oral as input. The reason we generated the new FastQC reports in the same directory is so that you can compare how the reads have changed after the quality filtering and decontamination steps in the same final multiqc report.\nmkdir -p /opt/results/final_multiqc_results/oral\n&lt;you construct the command&gt;\n\n\n\n\n\n\n\n\nRun multiQC (code)\n\n\n\n\n\nmkdir -p /opt/results/final_multiqc_results/oral\nmultiqc /opt/results/fastqc_results/oral -o /opt/results/final_multiqc_results/oral\n\n\n\n\n\n\n\n\n\nCheck report\n\n\n\nView the MultiQC report as before using your browser.\n\n\n\nScreenshot of multiQC\n\n\nScroll down through the report. The sequence quality histograms show the above results from each file as four separate lines. The ‘Status Checks’ show a matrix of which samples passed check and which ones have problems.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of the change in sequence quality histograms? Have they improved?\nDid sequences at the 5’ end become uniform? Why might that be? Is there anything that suggests that adaptor sequences were found?\n\n\nThe reads have now been decontaminated and can be uploaded to ENA, one of the INSDC members. It is beyond the scope of this course to include a tutorial on how to submit to ENA, but there is additional information available on how to do this in this Online Training guide provided by EMBL-EBI",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#assembly-phix-decontamination",
    "href": "sessions/qc.html#assembly-phix-decontamination",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Assembly PhiX decontamination",
    "text": "Assembly PhiX decontamination\n\n\n\n\n\n\nLearning Objectives\n\n\n\nIn the following exercises, you will generate a PhiX BLAST database and run a BLAST search with a subset of assembled freshwater sediment metagenomic reads to identify contamination.\n\n\nPhiX, used in the previous section of this practical, is a small bacteriophage genome typically used as a calibration control in sequencing runs. Most library preparations will use PhiX at low concentrations; however, it can still appear in the sequencing run. If not filtered out, PhiX can form small spurious contigs that could be incorrectly classified as diversity.\n\n\n\n\n\n\nGenerate the PhiX reference BLAST database:\n\n\n\nmakeblastdb -in /opt/data/decontamination/phix.fasta -input_type fasta -dbtype nucl -parse_seqids -out /opt/data/decontamination/phix_blastDB\n\n\nPrepare the freshwater sediment example assembly file and search against the new BLAST database. This assembly file contains only a subset of the contigs for the purpose of this practical.\n\n\n\n\n\n\nRun\n\n\n\nmkdir -p /opt/results/blast_results\ngunzip /opt/data/freshwater_sediment_contigs.fa.gz\nblastn -query /opt/data/freshwater_sediment_contigs.fa -db /opt/data/decontamination/phix_blastDB -task megablast -word_size 28 -best_hit_overhang 0.1 -best_hit_score_edge 0.1 -dust yes -evalue 0.0001 -min_raw_gapped_score 100 -penalty -5 -soft_masking true -window_size 100 -outfmt 6 -out /opt/results/blast_results/freshwater_blast_out.txt\n\n\nThe BLAST options are:\n\n-query — Input assembly fasta file.\n-out — Output file\n-db — Path to BLAST database.\n-task — Search type -“megablast”, for very similar sequences (e.g, sequencing errors)\n-word_size — Length of initial exact match\n\n\n\n\n\n\n\nAdd headers to the blast output and look at the contents of the final output file:\n\n\n\ncat /opt/data/blast_outfmt6.txt /opt/results/blast_results/freshwater_blast_out.txt &gt; /opt/results/blast_results/freshwater_blast_out_headers.txt\ncat /opt/results/blast_results/freshwater_blast_out_headers.txt\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre the hits significant?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the lengths of the matching contigs? We would typically filter metagenomic contigs at a length of 500bp. Would any PhiX contamination remain after this filter?\n\n\nNow that PhiX contamination was identified, it is important to remove these contigs from the assembly file before further analysis or upload to public archives. Just like you learnde in the last section, you could use a tool like bowtie2 to achieve this.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#using-negative-controls",
    "href": "sessions/qc.html#using-negative-controls",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Using Negative Controls",
    "text": "Using Negative Controls\n\n\n\n\n\n\nLearning Objectives\n\n\n\nThis exercise will look at the analysis of negative controls. You will assess the microbial diversity between a negative control and a skin sample.\n\n\nThe images below show the taxonomic classification of two samples: a reagent negative control and a skin metagenomic sample. The skin sample is taken from the antecubital fossa - the elbow crease, which is moist and a site of high microbial diversity. The classification was performed using a tool called Kraken2. Kraken2 takes a while to run, so we have done this for you and plotted the results. An example of the command used to do this. DO NOT run this now:\n\nkraken2 --db standard_db --threshold 0.10 --threads 8 --use-names --fastq-input --report out.report --gzip-compressed in_1.fastq.gz in_2.fastq.gz See the kraken2 manual for more information\n\nSee Pavian manual for the plots.\nThe following image shows the microbial abundance in the negative control: \nThe following image shows the microbial abundance in the skin sample: \n\n\n\n\n\n\nStep\n\n\n\nLook for similarities and differences at both the phylum and genus level - labelled as ‘P’ and ‘G’ on the bottom axis.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any overlap between the negative control and skin sample phylum? Can we map the negative control directly to the skin sample to remove all contaminants? If not, why?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any genera in the negative control which aren’t present in the skin sample? If you do a google search of this genus, where are they commonly found? With this information, where could this bacteria in the negative control have originated from?",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#additional-exercise",
    "href": "sessions/qc.html#additional-exercise",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Additional Exercise",
    "text": "Additional Exercise\nIf you have finished the practical you can try this step for more practice assessing and trimming datasets, there is another set of raw reads called “skin_example_aa” from the skin metagenome available. These will require a FastQC or multiqc report, followed by quality filtering and mapping to the reference database with fastp and bowtie2. Using what you have learned previously, construct the relevant commands. Remember to check the quality before and after trimming.\n\n\n\n\n\n\nNavigate to skin folder and run quality control.\n\n\n\nls /opt/data/skin\n# Output: skin_example_aa_1.fastq.gz  skin_example_aa_2.fastq.gz  skin_neg_control.fastq.gz\n\n\nRemember you will need to run the following command to view any html files in the VM browsers:\nchown 1001 foldername/*.html\n\n\n\n\n\n\nAdditional Excercise (code)\n\n\n\n\n\n\n\n#generate fastqc of raw reads of skin samples and negative control\nmkdir /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_1.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_example_aa_2.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/data/skin/skin_neg_control.fastq.gz -o /opt/results/fastqc_results/skin\n\n#do quality filtering using fastp \nmkdir /opt/results/cleaned/skin\nfastp --in1 /opt/data/skin/skin_example_aa_1.fastq.gz \\\n      --in2 /opt/data/skin/skin_example_aa_2.fastq.gz \\\n      --out1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n      --out2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n      -l 50 --cut_right --cut_right_window_size 4 --cut_right_mean_quality 20 -t 1 \\\n      --detect_adapter_for_pe \\\n      --json /opt/results/cleaned/skin/oral.fastp.json --html /opt/results/cleaned/skin/skin.fastp.html\n\n#do host decontamination with bowtie2\nbowtie2 -1 /opt/results/cleaned/skin/skin_example_aa_1.trimmed.fastq.gz \\\n        -2 /opt/results/cleaned/skin/skin_example_aa_2.trimmed.fastq.gz \\\n        -x /opt/data/decontamination/chr10_phix.index \\\n        --un-conc-gz  /opt/results/cleaned/skin/skin_human_example.fastq.gz \\\n        --very-sensitive --dovetail &gt; /dev/null\n\n##decontaminated reads will be output as skin_human_example.fastq.1.gz and skin_human_example.fastq.2.gz in the /opt/results/cleaned/skin/ folder\n\n#rename decontaminated reads to be consistent\nmv /opt/results/cleaned/skin/skin_human_example.fastq.1.gz /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz\nmv /opt/results/cleaned/skin/skin_human_example.fastq.2.gz /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz\n\n#post-qc assessment with Fastqc and MultiQC\nfastqc /opt/results/cleaned/skin/skin_human_example_1_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nfastqc /opt/results/cleaned/skin/skin_human_example_2_trimmed_decontam.fastq.gz -o /opt/results/fastqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#generate MultiQC report of pre- and post-QC fastq files\nmultiqc /opt/results/fastqc_results/skin -o /opt/results/multiqc_results/skin\nchown 1001 /opt/results/fastqc_results/skin/*.html\n\n#visualise multiQC reports in web browser.",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/qc.html#running-the-practical-externally",
    "href": "sessions/qc.html#running-the-practical-externally",
    "title": "Quality control and filtering of the raw sequence files",
    "section": "Running the practical externally",
    "text": "Running the practical externally\nWe need to first fetch the practical datasets.\nmkdir QC_session\ncd QC_session\n\nwget http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz\n# or\nrsync -av --partial --progress rsync://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_courses/ebi_2020/quality.tar.gz .\nOnce downloaded, extract the files from the tarball:\ntar -xzvf quality.tar.gz\n\nrm quality.tar.gz\n\nmkdir qc_results\n\ncd quality\nNow pull the docker container and export the above directories.\ndocker pull quay.io/microbiome-informatics/qc-practical-2024:v2\nexport DATADIR={path to quality directory}\nexport RESDIR={path to qc_results directory}\nYou will see the message “access control disabled, clients can connect from any host”\ndocker run --rm -it  -e DISPLAY=$DISPLAY  -v $DATADIR:/opt/data -v $RESDIR:/opt/results -v /tmp/.X11-unix:/tmp/.X11-unix:rw -e DISPLAY=unix$DISPLAY quay.io/microbiome-informatics/qc-practical-2024:v2\nThe container has the following tools installed:\n\nfastqc\nmultiqc\nfastp\nbowtie2\nblast\n\nYou can now continue this practical from the section “Quality control and filtering of the raw sequence files”",
    "crumbs": [
      "Home",
      "Sessions",
      "Quality control and filtering of the raw sequence files"
    ]
  },
  {
    "objectID": "sessions/getting_data_from_mgnify.html",
    "href": "sessions/getting_data_from_mgnify.html",
    "title": "Getting data from MGnify",
    "section": "",
    "text": "Exploring MGnify Studies\nExploring MGnify Analyses\nMetagenomic Assembled Genomes\n\n\n\nFor this tutorial, you will need to start the predefined Jupyter notebook and complete the exercises within it. follow the steps below To start the notebook.\n\nFrom your terminal, navigate to the right environment: cd /home/training/venv/training_env3\nActivate the environment: source bin/activate\nNavigate to the mgnify-tutorial directory: cd /home/training/venv/training_env3/mgnify-tutorial\nStart the Jupyter notebook: jupyter notebook\nThis should have a server started at http://localhost:8888. Open this in your browser.\nClick on content and then on the tutorial.ipynb notebook as shown in the images below to start the tutorial.\n\n\n\n\nJupyter Notebook\n\n\n\n\n\nJupyter Notebook",
    "crumbs": [
      "Home",
      "Sessions",
      "Getting data from MGnify"
    ]
  },
  {
    "objectID": "sessions/getting_data_from_mgnify.html#instructions",
    "href": "sessions/getting_data_from_mgnify.html#instructions",
    "title": "Getting data from MGnify",
    "section": "",
    "text": "For this tutorial, you will need to start the predefined Jupyter notebook and complete the exercises within it. follow the steps below To start the notebook.\n\nFrom your terminal, navigate to the right environment: cd /home/training/venv/training_env3\nActivate the environment: source bin/activate\nNavigate to the mgnify-tutorial directory: cd /home/training/venv/training_env3/mgnify-tutorial\nStart the Jupyter notebook: jupyter notebook\nThis should have a server started at http://localhost:8888. Open this in your browser.\nClick on content and then on the tutorial.ipynb notebook as shown in the images below to start the tutorial.\n\n\n\n\nJupyter Notebook\n\n\n\n\n\nJupyter Notebook",
    "crumbs": [
      "Home",
      "Sessions",
      "Getting data from MGnify"
    ]
  },
  {
    "objectID": "sessions/assemblies.html",
    "href": "sessions/assemblies.html",
    "title": "Assembly and Co-assembly of Metagenomic Raw Reads",
    "section": "",
    "text": "Learning Objectives\nIn the following exercises you will learn how to perform metagenomic assembly and co-assembly, and to start exploring the output. We will shortly observe assembly graphs with Bandage, peek into assembly statistics with assembly_stats, and align contig files against the BLAST database.\n\n\n\n\n\n\nNote\n\n\n\nThe process of metagenomic assembly can take hours, if not days, to complete on a normal sample, as it often requires days of CPU time and 100s of GB of memory. In this practical, we will only investigate very simple example datasets.\n\n\nOnce you have quality filtered your sequencing reads, you may want to perform de novo assembly in addition to, or as an alternative to, read-based analyses. The first step is to assemble your sequences into contigs. There are many tools available for this, such as MetaVelvet, metaSPAdes, IDBA-UD, or MEGAHIT.\nWe generally use metaSPAdes, as in most cases it yields the best contig size statistics (i.e. more contiguous assembly), and it has been shown to be able to capture high degrees of community diversity (Vollmers, et al. PLOS One 2017). However, you should consider pros and cons of different assemblers, which not only includes the accuracy of the assembly, but also their computational overhead. Compare these factors to what you have available.\nFor example, very diverse samples with a lot of sequence data (e.g. samples from the soil) uses a lot of memory with SPAdes. In the following practicals we will demonstrate the use of metaSPAdes on a small short-read sample, Flye on a long-read sample, and MEGAHIT to perform co-assembly.\n\n\nBefore we start…\nLet’s first move to the root working directory to run all analyses:\ncd /home/training/Assembly/\nActivate the environment as follows to access the tools we will use in this session:\nsource /home/training/venv/training_env1/bin/activate\nThe raw reads used for assembly generation can be found in the data_dir/[long/short]_reads folders.\n\n\nShort-read assemblies: metaSPAdes\nFor short reads, we will use SPAdes - St. Petersburg genome Assembler, a suite of assembling tools containing different assembly pipelines. For metagenomic data, we will use the more metagenomic-specific side of SPAdes: metaSPAdes.\nmetaSPAdes offers many options that fit your requirements differently, which mostly depend on the type of data you want to assemble. To explore them, type metaspades.py -h. Bear in mind that options will differ when selecting different tools (e.g. spades.py vs metaspades.py) and they should be tuned according to the input dataset and desired outcome.\n\n\n\n\n\n\nTip\n\n\n\nThe default metaSPAdes pipeline executes an error correction step on the input fastqs. Since reads have already been polished in the previous step, you can run metaSPAdes without the error correction step.\nYou can see available metaspades parameters by typing the following:\nmetaspades.py -h \n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis execution should be able to run on a 4-core, 16 GB RAM machine. However, we kindly ask you to NOT launch the execution on your VMs. Execution time would overall cover half of the session (metaSPAdes alone would take ~15 minutes). Also, you will prevent overload on the VMs and potential crashing, since VMs available resources barely cover metaSPAdes’ requirements.\n\n\nAn explanation of selected parameters follows:\n\n-t 4 threads\n--only-assembler skips the error correction step\n-m 5 memory limit in Gb\n-1 short_reads/input_1.fastq forward reads\n-2 short_reads/input_2.fastq reverse reads\n-o assembly_spades output folder\n\nAll output files (including intermediate ones) can be found in the assembly_spades folder. contigs.fasta and scaffolds.fasta are usually used for downstream analyses (e.g. binning and MAG generation). We will focus on contigs.fasta for this session, which is the same you are going to use in the coming practicals.\nWithout having to go all the way down to MAGs, you can sometimes identify strong taxonomic signals at the assembly stage with a quick blastn alignment.\n\n\n\n\n\n\nStep\n\n\n\nTake the first 100 lines of the sequence and perform a blast search (choose Nucleotide:Nucleotide from the set of options). Leave all other options as default on the search page. To select the first 100 lines of the assembly perform the following:\nhead -n 101 assembly_spades/contigs.fasta\nThe resulting output is going to have the following format (but look a bit better than this): \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat species does this sequence seem to be coming from?\nDoes this make sense as a human oral bacterium? Are you surprised by this result at all?\n\n\nAs mentioned in the theory talk, you might be interested in different statistics for your contigs. assembly_stats is a tool that produces two simple tables in JSON format with various measures, including N10 to N50, GC content, longest contig length and more. The first section of the JSON corresponds to the scaffolds in the assembly, while the second corresponds to the contigs.\n\n\n\n\n\n\nTip\n\n\n\nContigs are ordered from the longest to the shortest.\n\n\nN50 is a measure to describe the quality of assembled genomes that are fragmented in contigs of different length, used to assess the sequence length of the shortest contig at 50% of the total assembly length (after sorting assembled contigs from longest to shortest).\nA (hopefully) clarifying picture to understand N50, where N50==60: \nEssentially, the higher this value, the better, as it means that longer contigs cover half of the final assembly, making it less fragmented. However, this only makes sense when thinking about alike metagenomes.\nNote that, like N50, other values can be considered e.g. N90 is the shortest contig length to cover 90 percent of the metagenome.\nYou can call assembly_stats with the following command:\n\n\n\n\n\n\nStep\n\n\n\nassembly_stats assembly_spades/scaffolds.fasta\n\n\nYou will see a short output with a few statistics for your assembly. In lines with format N50 = YYY, n = Z, n represents the amount of sequences needed to cover 50% of the total assembly. A “gap” is any consecutive run of Ns (undetermined nucleotide bases) of any length. N_count is the total Ns (undetermined nucleotide bases) across the entire assembly.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the length of the longest and the shortest contigs?\nWhat is the N50 of the assembly? Given that input sequences were ~150bp long paired-end sequences, what does this tell you about the assembly in terms of statistics?\n\n\nAnother tool to keep in mind for metagenomic assemblies is QUAST, which provides a deeper insight on assemblies statistics, like indels and misassemblies rate, in a very short time.\n\n\nLong-read assemblies: Flye\nFor long-reads, we will use Flye, which assembles single-molecule sequencing reads like PacBio and Oxford Nanopore Technologies (ONT) reads. Like SPAdes, Flye is a pipeline that takes care of assembling raw reads and polishing. Similarly to assembly scaffolding, it tries to overcome long-read base call error by comparing different reads that cover the same sequencing fragment.\n\n\n\n\n\n\nTip\n\n\n\nFlye’s parameters are described in the help command:\nflye -h\nFlye supports metagenomic assemblies with the --meta flag.\n\n\nThe output for this section can be found in the assembly_flye folder. They have been generated with the following parameters:\n\n--nano-raw: depending on the quality of data, a different pre-set can be used. For example, if reads were previously polished and adapters were removed, we could use --nano-corr. --nano-hq should be reserved, respectively, for corrected and higher-quality data. The same applies to the pacbio options.\nlong_reads/file.fastq input raw reads file\n--out-dir assembly_flye output folder\n--threads 4 number of threads\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we did not use the --meta flag. The assembly output of the same command with the addition of --meta is in the folder assembly_flye_meta.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs before, we recommend to NOT launch this command during this session. Each execution would take around 5 minutes, but it would take up all CPUs.\n\n\n\n\nDiving into assembly graphs\nLet’s have a first look at how assembly graphs look like. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a program that creates interactive visualisations of assembly graphs. They can be useful for finding sections of the graph, such as rRNA, SNPs, or identify specific parts of a genome. Note, you can install Bandage on your local system. With Bandage, you can zoom and pan around the graph and search for sequences, and much more.\nWe will use Bandage to compare the two assembly graphs we generated with Flye and metaSPAdes. We will start with metaSPAdes.\nWhen looking at metaSPAdes files, it is usually recommended to launch Bandage on assembly_graph.fastg. However, our assembly is quite fragmented, so we will load assembly_graph_after_simplification.gfa.\n\n\n\n\n\n\nStep\n\n\n\nBandage &\nIn the Bandage GUI perform the following:\n\nSelect File -&gt; Load graph\nNavigate to /home/training/Assembly/assembly_spades and open assembly_graph_after_simplification.gfa\n\nOnce loaded, you need to draw the graph. To do so, under the “Graph drawing” panel on the left side perform the following:\n\nSet Scope to Entire graph\nClick on Draw graph\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you find any large, complex parts of the metaSPAdes graph? If so, what do they look like?\n\n\nNow open another instance of Bandage as you previously did, and open assembly_flye/assembly_graph.gfa.\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the Flye assembly differ from the one generated with metaSPAdes?\n\n\nAs already mentioned, we launched Flye both with and without --meta on long_reads/ONT_example.fastq. You can now repeat the procedure for this other graph.\n\n\n\n\n\n\nExtra\n\n\n\nThis file originally comes from run ERR3775163, which can be browsed on ENA. Have a look at its metadata.\nCan you understand why the assembly graph of the execution without --meta looks better than the one with --meta?\n\n\n\n\n\n\n\n\nExtra\n\n\n\nIf you blast the first contig of the long-read assembly, do results match the metadata you find on ENA?\n\n\n\n\nCo-assemblies: MEGAHIT\nIn the following steps of this exercise, we will observe co-assembly of multiple datasets. Remember that co-assembly produces meaningful results only when applied to “similar” samples. This is the case for the raw reads we co-assembled: they originally come from a single sample that has been split for this exercise. In particular, we co-assembled more data coming from the same sample we assembled with metaSPAdes.\nmegahit -h\n\n\n\n\n\n\nTip\n\n\n\nYou will find MEGAHIT output files in the co_assembly_short_reads folder.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs earlier, these are the parameters we generated the co-assemblies with, but we recommend not to launch the command in this instance.\n\n\n\n-1 [forward files comma-separated]\n-2 [reverse files comma-separated]\n-o co_assembly_megahit output folder\n-t 4 number of threads\n--k-list 23,51,77 list of k-mer lengths\n\n\n\n\n\n\n\nStep\n\n\n\nCompare the results of the co-assembly contig file to the single-assembly one with assembly_stats.\nHow do this assembly differ from the one generated previously with metaSPAdes? Which one do you think is best?\n\n\n\n\n\n\n\n\nExtra\n\n\n\nYou might notice that MEGAHIT does not generate assembly graphs by default. To do so, execute:\nmegahit_toolkit contig2fastg 77 co_assembly_megahit/final.contigs.fa &gt; co_assembly_megahit/final.contigs.fastg\nThe samples used for the metaSPAdes assembly and the MEGAHIT co-assembly come from the same source. Do you see any relevant difference between the two assembly graphs?\n\n\n\n\n… And now?\nIf you have reached the end of the practical and have some spare time, look at the paragraphs labelled “Extra”. They contain optional exercises for the curious student :)\n\n\n…….. Yes, but now that I am really, really done?\nYou could try to assemble raw reads with different assemblers or parameters, and compare statistics and assembly graphs. Note, for example, that metaSPAdes can deal ONT data (but it will likely yield a lower quality assembly).\n\n\n\n\nReuseApache 2.0",
    "crumbs": [
      "Home",
      "Sessions",
      "Assembly and Co-assembly of Metagenomic Raw Reads"
    ]
  }
]